{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1a_JFyoHgKDTCukwSGYD4cmUTcoOh2q2M","timestamp":1746962330281}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3423a9b6255b4e63aed7b2f9501e35fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_354e2383f6424727a5c4fe2859914c9b","IPY_MODEL_70786911e0184e058152e5197337bdeb","IPY_MODEL_abf3fedc54484314a5b38cf14a382b1f"],"layout":"IPY_MODEL_5722e0e0bc524199a658c718e09e28ff"}},"354e2383f6424727a5c4fe2859914c9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3679bf43d1434f0e8dc5338f2a2b4661","placeholder":"​","style":"IPY_MODEL_ca7358343cb1403fb91e63da30a83669","value":"README.md: 100%"}},"70786911e0184e058152e5197337bdeb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceeca6f7d29c4d40be169c3a59922d74","max":4028,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ec45a1da9c64df4b62a8d1cb4599ed0","value":4028}},"abf3fedc54484314a5b38cf14a382b1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_883fb6beb2dd415a875165f37173d099","placeholder":"​","style":"IPY_MODEL_5dc7d43a11f746bebf76176b1e0aed6c","value":" 4.03k/4.03k [00:00&lt;00:00, 144kB/s]"}},"5722e0e0bc524199a658c718e09e28ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3679bf43d1434f0e8dc5338f2a2b4661":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca7358343cb1403fb91e63da30a83669":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceeca6f7d29c4d40be169c3a59922d74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ec45a1da9c64df4b62a8d1cb4599ed0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"883fb6beb2dd415a875165f37173d099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dc7d43a11f746bebf76176b1e0aed6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f450a67668334be0bfc634b1a00c6053":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3bc7855aed44dfc82cf6fcadc37eeeb","IPY_MODEL_ba82787f6aa3406db7a33dbc32259c0b","IPY_MODEL_616cd9a9be02416db00bb56cf6eafa42"],"layout":"IPY_MODEL_fbce241a80ed4c95a6aa17b5df7fc3c3"}},"b3bc7855aed44dfc82cf6fcadc37eeeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15bfd9a6e21a40fe857553bb49c29d8f","placeholder":"​","style":"IPY_MODEL_96a1b966a732441487e97def4a07ed9b","value":"train-00000-of-00011.parquet: 100%"}},"ba82787f6aa3406db7a33dbc32259c0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e57efbcafd641fda517efd0eff4fcaf","max":450894905,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c42eef1278d44df0942dce2479d5b531","value":450894905}},"616cd9a9be02416db00bb56cf6eafa42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f366bd736bed4f05952b94c276e0e992","placeholder":"​","style":"IPY_MODEL_7040058262404c66bc6917d5117c4960","value":" 451M/451M [00:01&lt;00:00, 241MB/s]"}},"fbce241a80ed4c95a6aa17b5df7fc3c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15bfd9a6e21a40fe857553bb49c29d8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a1b966a732441487e97def4a07ed9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e57efbcafd641fda517efd0eff4fcaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c42eef1278d44df0942dce2479d5b531":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f366bd736bed4f05952b94c276e0e992":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7040058262404c66bc6917d5117c4960":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96caa9d774f04604be6e347ed9e230fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93967097184045f986f5002663e6351e","IPY_MODEL_22e36b0d74374a4297ea890c47e74405","IPY_MODEL_3a361d77921843e48c983abddd71e095"],"layout":"IPY_MODEL_82d2e14a3d144f99be07911d1aab7090"}},"93967097184045f986f5002663e6351e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7890c66b68e445593adf36c6a83549f","placeholder":"​","style":"IPY_MODEL_f868237c94d54e20b7593a567b8131dd","value":"train-00001-of-00011.parquet: 100%"}},"22e36b0d74374a4297ea890c47e74405":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3900ba35354b4239b5ab233d9e653dcf","max":452127129,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a51378f5d174b6c9558479b3bf64a0f","value":452127129}},"3a361d77921843e48c983abddd71e095":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4b6e076c3f04f94a94465826fe4fe01","placeholder":"​","style":"IPY_MODEL_2cf28e168f804fed9072dc9dfc06ab0c","value":" 452M/452M [00:01&lt;00:00, 248MB/s]"}},"82d2e14a3d144f99be07911d1aab7090":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7890c66b68e445593adf36c6a83549f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f868237c94d54e20b7593a567b8131dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3900ba35354b4239b5ab233d9e653dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a51378f5d174b6c9558479b3bf64a0f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4b6e076c3f04f94a94465826fe4fe01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cf28e168f804fed9072dc9dfc06ab0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4158f40f67964a158e1557e129722672":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f3835724dba407899b601d6cc039b6b","IPY_MODEL_6d0cebd30099424a86b508546b565e9c","IPY_MODEL_d96b3fcf3cf043c2bd7bcb0bcfa920a4"],"layout":"IPY_MODEL_d362ff5ae594474ea6815d29a0034ec3"}},"4f3835724dba407899b601d6cc039b6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7430f14a2084698a3ad7ef35b49f2bc","placeholder":"​","style":"IPY_MODEL_52538d913bd94f1f97843dad45164e32","value":"train-00002-of-00011.parquet: 100%"}},"6d0cebd30099424a86b508546b565e9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca8e1060d4464b2aa517b50c22c1ca41","max":456191024,"min":0,"orientation":"horizontal","style":"IPY_MODEL_005f827fdd5c4217a99709113c57a332","value":456191024}},"d96b3fcf3cf043c2bd7bcb0bcfa920a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49a2fe799ea24b2b808f0007c0c03251","placeholder":"​","style":"IPY_MODEL_301ee82e3c884177b69a0f7effbf5cf7","value":" 456M/456M [00:05&lt;00:00, 182MB/s]"}},"d362ff5ae594474ea6815d29a0034ec3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7430f14a2084698a3ad7ef35b49f2bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52538d913bd94f1f97843dad45164e32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca8e1060d4464b2aa517b50c22c1ca41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"005f827fdd5c4217a99709113c57a332":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49a2fe799ea24b2b808f0007c0c03251":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"301ee82e3c884177b69a0f7effbf5cf7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8cccfa9bce4e4fbeb5ff026834739da0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1f701e21259444c8e5aad1089419eb0","IPY_MODEL_6d3d487521714c64b39302fede1f84c9","IPY_MODEL_45c2ea6f547e4a7e8aca813688194456"],"layout":"IPY_MODEL_d63a8256a9204af789a20e9c2d2a36bf"}},"e1f701e21259444c8e5aad1089419eb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_658e45df03f745a4863400f994fba5c1","placeholder":"​","style":"IPY_MODEL_eb612a7909744d4cab11e1605041550b","value":"train-00003-of-00011.parquet: 100%"}},"6d3d487521714c64b39302fede1f84c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46ca6ab6f2a04c6eb8f9a2a9c65d8ae0","max":449378782,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec85070387934860b00a9c8dc805fc11","value":449378782}},"45c2ea6f547e4a7e8aca813688194456":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb40b0fba4ba4f94b6eec2ac7c0df182","placeholder":"​","style":"IPY_MODEL_7eaeaa055e7d4d509fb609f9fba497d0","value":" 449M/449M [00:02&lt;00:00, 165MB/s]"}},"d63a8256a9204af789a20e9c2d2a36bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"658e45df03f745a4863400f994fba5c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb612a7909744d4cab11e1605041550b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46ca6ab6f2a04c6eb8f9a2a9c65d8ae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec85070387934860b00a9c8dc805fc11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb40b0fba4ba4f94b6eec2ac7c0df182":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eaeaa055e7d4d509fb609f9fba497d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2fdf853b16c48c4aa0e8725ffc6b130":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3f6b5f229024caeb0003898187a6efa","IPY_MODEL_cf836ee0fcdd42d49f4cdeeafa3e3050","IPY_MODEL_3aea2453b5e445eb85a10df16541f35e"],"layout":"IPY_MODEL_55f08a97fa3d4c2693b42c07c8b5a218"}},"f3f6b5f229024caeb0003898187a6efa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_839b17257ef346b3b8ae5afa9f8b9bda","placeholder":"​","style":"IPY_MODEL_3026791b06114e13ae27af20667e252e","value":"train-00004-of-00011.parquet: 100%"}},"cf836ee0fcdd42d49f4cdeeafa3e3050":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24de7ba49cac41378b32fd4dc6dbb301","max":455001339,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87bac54cde4843cf80bc0b28ee6a5631","value":455001339}},"3aea2453b5e445eb85a10df16541f35e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6dc082b2c904bf1ae1222226cec5793","placeholder":"​","style":"IPY_MODEL_bfc6aee65c934919adc711403e09575a","value":" 455M/455M [00:01&lt;00:00, 242MB/s]"}},"55f08a97fa3d4c2693b42c07c8b5a218":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"839b17257ef346b3b8ae5afa9f8b9bda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3026791b06114e13ae27af20667e252e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24de7ba49cac41378b32fd4dc6dbb301":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87bac54cde4843cf80bc0b28ee6a5631":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6dc082b2c904bf1ae1222226cec5793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfc6aee65c934919adc711403e09575a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bd1aa1c71b347699500b590f3aacc2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_195a3c59763a4a229054ce327e7d13df","IPY_MODEL_d5008330ad1344369c687af81ee98034","IPY_MODEL_d665047dec7f482e818a31df6d31c1e4"],"layout":"IPY_MODEL_d4b119f74c1241bf866a4429a585c6df"}},"195a3c59763a4a229054ce327e7d13df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15c1a7b51a3345368542d6b72bc3811e","placeholder":"​","style":"IPY_MODEL_8930cc34626c4aaeb59b91fd0c287b4c","value":"train-00005-of-00011.parquet: 100%"}},"d5008330ad1344369c687af81ee98034":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7213d6a88ca54a89af4d3ac3a36456ab","max":452935330,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de5c719d39ec465487a783ba3b40c36a","value":452935330}},"d665047dec7f482e818a31df6d31c1e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_351aeb359f5b465ba294e0399a1cd951","placeholder":"​","style":"IPY_MODEL_6ff2ad315edb481ca67a6ad246e77f33","value":" 453M/453M [00:02&lt;00:00, 189MB/s]"}},"d4b119f74c1241bf866a4429a585c6df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15c1a7b51a3345368542d6b72bc3811e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8930cc34626c4aaeb59b91fd0c287b4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7213d6a88ca54a89af4d3ac3a36456ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de5c719d39ec465487a783ba3b40c36a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"351aeb359f5b465ba294e0399a1cd951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ff2ad315edb481ca67a6ad246e77f33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ed5718e3f7c4b2384bd398f3e06c3bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_137a51113e1c4a32af800ecbf1b5c31e","IPY_MODEL_c6eb2c43ba9a4c1a99d7a58a7a980d04","IPY_MODEL_39b1b28db89840d1867772332ff4a4ad"],"layout":"IPY_MODEL_fe08bf8457e643dfa2e243783e8da86e"}},"137a51113e1c4a32af800ecbf1b5c31e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d62485e8fe364432ab70dfc2136d5dc1","placeholder":"​","style":"IPY_MODEL_b7d8fe44f6224c66927f31d405a80b91","value":"train-00006-of-00011.parquet: 100%"}},"c6eb2c43ba9a4c1a99d7a58a7a980d04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0f8a283f46b41e88f4318d7e9e3ab70","max":452017185,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d324822572d544f4beed5483dea0c137","value":452017185}},"39b1b28db89840d1867772332ff4a4ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfb74b71b86e44da94e9c5197dc08b0a","placeholder":"​","style":"IPY_MODEL_ed8c5e33578c4b06bcfb7365df17f62d","value":" 452M/452M [00:01&lt;00:00, 233MB/s]"}},"fe08bf8457e643dfa2e243783e8da86e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d62485e8fe364432ab70dfc2136d5dc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7d8fe44f6224c66927f31d405a80b91":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0f8a283f46b41e88f4318d7e9e3ab70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d324822572d544f4beed5483dea0c137":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfb74b71b86e44da94e9c5197dc08b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed8c5e33578c4b06bcfb7365df17f62d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6dda7c5a21764bf0b78f0e009ec3bb94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c850f86868e94938ae5dd25688a1700a","IPY_MODEL_6c8c17ff01994280aa237372a7aa4953","IPY_MODEL_365beb9e232a474da76d0eb883a1a509"],"layout":"IPY_MODEL_06f34bd69b074d96a4949917c0a1f4f9"}},"c850f86868e94938ae5dd25688a1700a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45821ba090af4d689afae1b8aa4b57d5","placeholder":"​","style":"IPY_MODEL_11b87603c63f4ed388bcec564dcecc66","value":"train-00007-of-00011.parquet: 100%"}},"6c8c17ff01994280aa237372a7aa4953":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c23a2f6edb549e891af94b1adc8453f","max":456696809,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b0847ac217b4ada956614b2ea4eac91","value":456696809}},"365beb9e232a474da76d0eb883a1a509":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4641c1acdc774fadbeca3d023747e505","placeholder":"​","style":"IPY_MODEL_8d682655d4e747c9ae3dc9c86792f195","value":" 457M/457M [00:04&lt;00:00, 145MB/s]"}},"06f34bd69b074d96a4949917c0a1f4f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45821ba090af4d689afae1b8aa4b57d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11b87603c63f4ed388bcec564dcecc66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c23a2f6edb549e891af94b1adc8453f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b0847ac217b4ada956614b2ea4eac91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4641c1acdc774fadbeca3d023747e505":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d682655d4e747c9ae3dc9c86792f195":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67765f027dcc457a872668a9fa5c76ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb9784b0f9034b3db9abfa9f6388edaf","IPY_MODEL_98e16956c4d24576910910a2d19a1198","IPY_MODEL_c3cba52a7db24652ab4a72294cddaea4"],"layout":"IPY_MODEL_ae7d9c36b78244b0a88d5868bfeba490"}},"fb9784b0f9034b3db9abfa9f6388edaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84698d9d279344778131d0d68313b36c","placeholder":"​","style":"IPY_MODEL_bc5363e13f3e4e4fb3302515266df7b0","value":"train-00008-of-00011.parquet: 100%"}},"98e16956c4d24576910910a2d19a1198":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_781895aa6b414034b278afedf1c863f0","max":452617361,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38b6c5dbd45840f18bf96725c3fc8ed2","value":452617361}},"c3cba52a7db24652ab4a72294cddaea4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d42bba59298452ab228fb168e3677ca","placeholder":"​","style":"IPY_MODEL_c80c3730f0dd43509aaf124278e34634","value":" 453M/453M [00:02&lt;00:00, 193MB/s]"}},"ae7d9c36b78244b0a88d5868bfeba490":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84698d9d279344778131d0d68313b36c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc5363e13f3e4e4fb3302515266df7b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"781895aa6b414034b278afedf1c863f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38b6c5dbd45840f18bf96725c3fc8ed2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d42bba59298452ab228fb168e3677ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c80c3730f0dd43509aaf124278e34634":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e6001c0284f461994f7ec76af7c7642":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_393c31d0548d44b99ea55bd31e0a0f05","IPY_MODEL_56fbc8457179456ca5bd33b942c129a1","IPY_MODEL_ef3a941f24b34f35b044ff9032e29885"],"layout":"IPY_MODEL_7472333ae2eb42ae8a6380502e1fa568"}},"393c31d0548d44b99ea55bd31e0a0f05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b19c19671cb4687821e92a67deef92f","placeholder":"​","style":"IPY_MODEL_8b8a10ed16c44bcb9c57009039662d73","value":"train-00009-of-00011.parquet: 100%"}},"56fbc8457179456ca5bd33b942c129a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f43f8805cd54779893bad54961f2318","max":452021401,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eec452a3b3f847a0844dfcc7a9c32044","value":452021401}},"ef3a941f24b34f35b044ff9032e29885":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9af9d3b51244a6c96b06af2509dd39d","placeholder":"​","style":"IPY_MODEL_60890527ad3f4f7a9500e3bd79715f11","value":" 452M/452M [00:02&lt;00:00, 204MB/s]"}},"7472333ae2eb42ae8a6380502e1fa568":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b19c19671cb4687821e92a67deef92f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b8a10ed16c44bcb9c57009039662d73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f43f8805cd54779893bad54961f2318":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eec452a3b3f847a0844dfcc7a9c32044":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9af9d3b51244a6c96b06af2509dd39d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60890527ad3f4f7a9500e3bd79715f11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4d1ea2470474602ad4b6b8f8db72638":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2c8fc615e494907b963bac09249f2f0","IPY_MODEL_6a441421035f4b9590fea7186acbd92b","IPY_MODEL_604eda3164ee456fa04798c7449c9855"],"layout":"IPY_MODEL_70fdc6f6b5e24ddea16f5e6a0dd624d8"}},"a2c8fc615e494907b963bac09249f2f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3ad867ea4b842fe91ab572d15edd6d9","placeholder":"​","style":"IPY_MODEL_2d3bb7fc2d17483bb062fccf88c577e7","value":"train-00010-of-00011.parquet: 100%"}},"6a441421035f4b9590fea7186acbd92b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd3ea5a03bc34036b86fe1b4dba2eeb4","max":450408603,"min":0,"orientation":"horizontal","style":"IPY_MODEL_727af5b3883440dca753c7486d252d97","value":450408603}},"604eda3164ee456fa04798c7449c9855":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2db243a4fa3d422f9c5445cf6af6fea5","placeholder":"​","style":"IPY_MODEL_5187a7b6586c43ffa7334bf060dc1761","value":" 450M/450M [00:02&lt;00:00, 215MB/s]"}},"70fdc6f6b5e24ddea16f5e6a0dd624d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3ad867ea4b842fe91ab572d15edd6d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d3bb7fc2d17483bb062fccf88c577e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd3ea5a03bc34036b86fe1b4dba2eeb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"727af5b3883440dca753c7486d252d97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2db243a4fa3d422f9c5445cf6af6fea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5187a7b6586c43ffa7334bf060dc1761":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a93a59da7504d3380efa62c17e7de7f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c24bc877fe374ab2b30326a88eec45af","IPY_MODEL_001650a5ce414da0a5fec1b045a538f9","IPY_MODEL_3a606736a4824e0db2c5dd5e471c954a"],"layout":"IPY_MODEL_ba51cddde50d42daa7241e8f25fb1f7c"}},"c24bc877fe374ab2b30326a88eec45af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b355c9ae2dc54f978702b633b5f86195","placeholder":"​","style":"IPY_MODEL_9a0487614b3144d18b5bae050209990f","value":"validation-00000-of-00001.parquet: 100%"}},"001650a5ce414da0a5fec1b045a538f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41bfdb674472434887df58987dbf5b6a","max":183565721,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63d05617a45740e099b4e74cf103bba9","value":183565721}},"3a606736a4824e0db2c5dd5e471c954a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c70ede7bdeac4f18b04e3e974c69c605","placeholder":"​","style":"IPY_MODEL_bd041a7f402c4551be5d354a709a8960","value":" 184M/184M [00:00&lt;00:00, 193MB/s]"}},"ba51cddde50d42daa7241e8f25fb1f7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b355c9ae2dc54f978702b633b5f86195":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a0487614b3144d18b5bae050209990f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41bfdb674472434887df58987dbf5b6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63d05617a45740e099b4e74cf103bba9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c70ede7bdeac4f18b04e3e974c69c605":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd041a7f402c4551be5d354a709a8960":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f620c1cea0f44994a65a6edcb4678317":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86c77e53a09244bd92aa2c0b481177c8","IPY_MODEL_fadc2b2349824394bd6f0b3aafb2af8f","IPY_MODEL_d19cea5a218f4ab09af64b8e544a59b5"],"layout":"IPY_MODEL_30f5ab997ae94ddf93dbdbb119859099"}},"86c77e53a09244bd92aa2c0b481177c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb127c246bea4dc28f860536812c0341","placeholder":"​","style":"IPY_MODEL_443e9b5d3d0841fe981e5e0d1fa30f86","value":"test-00000-of-00002.parquet: 100%"}},"fadc2b2349824394bd6f0b3aafb2af8f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6556310a79544a6d86a709f3b1a988e6","max":315796525,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d64f3a1f0ce446cac0436a836e8e768","value":315796525}},"d19cea5a218f4ab09af64b8e544a59b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43907c5a148e4a93a579fd0f06d0dd86","placeholder":"​","style":"IPY_MODEL_67d75125c1a649c6abffd1e4042f3b36","value":" 316M/316M [00:01&lt;00:00, 214MB/s]"}},"30f5ab997ae94ddf93dbdbb119859099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb127c246bea4dc28f860536812c0341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"443e9b5d3d0841fe981e5e0d1fa30f86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6556310a79544a6d86a709f3b1a988e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d64f3a1f0ce446cac0436a836e8e768":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43907c5a148e4a93a579fd0f06d0dd86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67d75125c1a649c6abffd1e4042f3b36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b3d30416ecd4ef4be770526635e1f4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0550c2eba01e41bc88dc87930d245f76","IPY_MODEL_22c4dbb28af74ffc92d063e548e2047f","IPY_MODEL_a3465a53e56a4726b04d64b106de7d14"],"layout":"IPY_MODEL_107c96ae31974dd092677a8fc7480b5b"}},"0550c2eba01e41bc88dc87930d245f76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2214192e62c4c0189b5c5b1e8a0e8cb","placeholder":"​","style":"IPY_MODEL_c6236fa8d02f4588982b671b0223c3be","value":"test-00001-of-00002.parquet: 100%"}},"22c4dbb28af74ffc92d063e548e2047f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18538b4e597d4287a229b920e3420e34","max":311988385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7910b26c70e741b298359934d3645575","value":311988385}},"a3465a53e56a4726b04d64b106de7d14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4be05a5f6de542a78f397c39726ee105","placeholder":"​","style":"IPY_MODEL_6663a01c91bb49c6b526e73d0a72cb1c","value":" 312M/312M [00:01&lt;00:00, 199MB/s]"}},"107c96ae31974dd092677a8fc7480b5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2214192e62c4c0189b5c5b1e8a0e8cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6236fa8d02f4588982b671b0223c3be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18538b4e597d4287a229b920e3420e34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7910b26c70e741b298359934d3645575":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4be05a5f6de542a78f397c39726ee105":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6663a01c91bb49c6b526e73d0a72cb1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e9340db94c9456fa3058c05837af0c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6529766373b4456b0ef3c70c39cf1a2","IPY_MODEL_a539d0718bd0426f9afb24742f970981","IPY_MODEL_46b13bb87f6848bfb887ab75bee963d4"],"layout":"IPY_MODEL_fc21aeaa24d34c69af6e8b9d3f61fc23"}},"b6529766373b4456b0ef3c70c39cf1a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7904b74e5a94a9383fe98439cdfe458","placeholder":"​","style":"IPY_MODEL_de1277e3bd69467c93a1f1b30d06623e","value":"Generating train split: 100%"}},"a539d0718bd0426f9afb24742f970981":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eddcb857bd44d3a9c98fa0965e1a7a7","max":54087,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9053c547ace4507a33d816681a36bad","value":54087}},"46b13bb87f6848bfb887ab75bee963d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc5d8d293e554f7b8c7860cfd7d6addb","placeholder":"​","style":"IPY_MODEL_90c7151ce643489bb8682413d5f9862f","value":" 54087/54087 [00:45&lt;00:00, 1672.09 examples/s]"}},"fc21aeaa24d34c69af6e8b9d3f61fc23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7904b74e5a94a9383fe98439cdfe458":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de1277e3bd69467c93a1f1b30d06623e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2eddcb857bd44d3a9c98fa0965e1a7a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9053c547ace4507a33d816681a36bad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc5d8d293e554f7b8c7860cfd7d6addb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90c7151ce643489bb8682413d5f9862f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0938a1b83f7942ad807be8e5fcb8351b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1e344241b5334032a90fb6722fbe438b","IPY_MODEL_7dadb91a174142af9a98e64d9df6142b","IPY_MODEL_177678b00b6a4f358e1f741ace9235c1"],"layout":"IPY_MODEL_86967d3715024cedb3624a84d05e67be"}},"1e344241b5334032a90fb6722fbe438b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7af1093689f4b209963c4a74e6408f1","placeholder":"​","style":"IPY_MODEL_ff35e6813ee6416b8682dafdfff312c9","value":"Generating validation split: 100%"}},"7dadb91a174142af9a98e64d9df6142b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_823e705ab04d4558890f466bfcce36fb","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed539569edad423e9dc625a80a42848b","value":2000}},"177678b00b6a4f358e1f741ace9235c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bbed3e6d17a4446ad5d459168c2f3fb","placeholder":"​","style":"IPY_MODEL_3ed437fb6ca2464aa09aebe80ce555fe","value":" 2000/2000 [00:01&lt;00:00, 1323.64 examples/s]"}},"86967d3715024cedb3624a84d05e67be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7af1093689f4b209963c4a74e6408f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff35e6813ee6416b8682dafdfff312c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"823e705ab04d4558890f466bfcce36fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed539569edad423e9dc625a80a42848b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bbed3e6d17a4446ad5d459168c2f3fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ed437fb6ca2464aa09aebe80ce555fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c6d94883fb44cbe877c78d68d35fad4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54970e3a30a448988fca43f5dc8c6642","IPY_MODEL_396db771e14d457188b126e3e21ef3ab","IPY_MODEL_4d4da668804048458d4c0615cf0b82fe"],"layout":"IPY_MODEL_4d134fbab06946afbecb6262dec84d3f"}},"54970e3a30a448988fca43f5dc8c6642":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e890cc58752c4b18a0f2653f7466120a","placeholder":"​","style":"IPY_MODEL_758b398a690448239de89aaa57ab7111","value":"Generating test split: 100%"}},"396db771e14d457188b126e3e21ef3ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b96644b9548492ba4d4e4c6e4061893","max":6854,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8512a35411c643398f0e37c57c29484c","value":6854}},"4d4da668804048458d4c0615cf0b82fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc22e3d2cbfe4a7f8c5831bb1cfe36cf","placeholder":"​","style":"IPY_MODEL_91ca476acfd54b29a00d70bbe0f50e98","value":" 6854/6854 [00:04&lt;00:00, 2942.43 examples/s]"}},"4d134fbab06946afbecb6262dec84d3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e890cc58752c4b18a0f2653f7466120a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"758b398a690448239de89aaa57ab7111":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b96644b9548492ba4d4e4c6e4061893":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8512a35411c643398f0e37c57c29484c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc22e3d2cbfe4a7f8c5831bb1cfe36cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91ca476acfd54b29a00d70bbe0f50e98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"qSnNCnmWouhz","outputId":"60c1c340-3a66-4e92-e035-d9598bad8411"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.95-py3-none-any.whl.metadata (35 kB)\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.95-py3-none-any.whl (949 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.95 ultralytics-thop-2.0.14\n","Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","ERROR: unknown command \"insall\" - maybe you meant \"install\"\n","ERROR: unknown command \"insall\" - maybe you meant \"install\"\n","Collecting tensorrt\n","  Downloading tensorrt-10.9.0.34.tar.gz (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorrt_cu12==10.9.0.34 (from tensorrt)\n","  Downloading tensorrt_cu12-10.9.0.34.tar.gz (18 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorrt_cu12_libs==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt)\n","  Downloading tensorrt_cu12_libs-10.9.0.34.tar.gz (704 bytes)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorrt_cu12_bindings==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt)\n","  Downloading tensorrt_cu12_bindings-10.9.0.34-cp311-none-manylinux_2_28_x86_64.whl.metadata (606 bytes)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from tensorrt_cu12_libs==10.9.0.34->tensorrt_cu12==10.9.0.34->tensorrt) (12.4.127)\n","Downloading tensorrt_cu12_bindings-10.9.0.34-cp311-none-manylinux_2_28_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt_cu12_libs\n","  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorrt: filename=tensorrt-10.9.0.34-py2.py3-none-any.whl size=46629 sha256=5a4320af4d4753c483ad82e640fd0f1388cfc72a1491bdcb6b65098adb4b4b84\n","  Stored in directory: /root/.cache/pip/wheels/3a/4d/72/f28cb367f1435d026243047d4f60fde8f1c9cbb06a204f842f\n","  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.9.0.34-py2.py3-none-any.whl size=17465 sha256=3aebc215ac36eeda529d5a1180e798a814a92353501aaf39577c5928c32bc77b\n","  Stored in directory: /root/.cache/pip/wheels/75/09/76/6b405075fe4c04097f5713ec0a688df7892aaee823bc141952\n","  Building wheel for tensorrt_cu12_libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorrt_cu12_libs: filename=tensorrt_cu12_libs-10.9.0.34-py2.py3-none-manylinux_2_28_x86_64.whl size=3103291777 sha256=4a82f0bda2874596f202f6edc8dae99b86a3c4ec2fa142a9c847c4d3a57864a0\n","  Stored in directory: /root/.cache/pip/wheels/33/d0/06/35d7b3006eead25828debb658da848328ebfd38962a2bcd096\n","Successfully built tensorrt tensorrt_cu12 tensorrt_cu12_libs\n","Installing collected packages: tensorrt_cu12_bindings, tensorrt_cu12_libs, tensorrt_cu12, tensorrt\n","Successfully installed tensorrt-10.9.0.34 tensorrt_cu12-10.9.0.34 tensorrt_cu12_bindings-10.9.0.34 tensorrt_cu12_libs-10.9.0.34\n","Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Collecting sng4onnx\n","  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n","Collecting onnx_graphsurgeon\n","  Downloading onnx_graphsurgeon-0.5.6-py2.py3-none-any.whl.metadata (8.2 kB)\n","Collecting onnx2tf\n","  Downloading onnx2tf-1.27.0-py3-none-any.whl.metadata (147 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.7/147.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tflite_support\n","  Downloading tflite_support-0.4.4-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from onnx_graphsurgeon) (2.0.2)\n","Collecting onnx>=1.14.0 (from onnx_graphsurgeon)\n","  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (1.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (25.2.10)\n","Collecting protobuf\n","  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n","Collecting sounddevice>=0.4.4 (from tflite_support)\n","  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Collecting pybind11>=2.6.0 (from tflite_support)\n","  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->tflite_support) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite_support) (2.22)\n","Downloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n","Downloading onnx_graphsurgeon-0.5.6-py2.py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnx2tf-1.27.0-py3-none-any.whl (446 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.2/446.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tflite_support-0.4.4-cp311-cp311-manylinux2014_x86_64.whl (60.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n","Installing collected packages: sng4onnx, pybind11, protobuf, onnx2tf, sounddevice, onnx, tflite_support, onnx_graphsurgeon\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.3\n","    Uninstalling protobuf-5.29.3:\n","      Successfully uninstalled protobuf-5.29.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed onnx-1.17.0 onnx2tf-1.27.0 onnx_graphsurgeon-0.5.6 protobuf-3.20.3 pybind11-2.13.6 sng4onnx-1.0.4 sounddevice-0.5.1 tflite_support-0.4.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"3d094cc335124a68bccdd998bdf3182b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip install ultralytics\n","!pip install gdown\n","!pip install tensorflow\n","!pip insall onnx\n","!pip insall onnxruntime\n","!pip install tensorrt\n","!pip install sng4onnx onnx_graphsurgeon onnx2tf tflite_support protobuf --extra-index-url https://pypi.ngc.nvidia.com\n","!pip install datasets"]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"HichTala/dota\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":717,"referenced_widgets":["3423a9b6255b4e63aed7b2f9501e35fa","354e2383f6424727a5c4fe2859914c9b","70786911e0184e058152e5197337bdeb","abf3fedc54484314a5b38cf14a382b1f","5722e0e0bc524199a658c718e09e28ff","3679bf43d1434f0e8dc5338f2a2b4661","ca7358343cb1403fb91e63da30a83669","ceeca6f7d29c4d40be169c3a59922d74","4ec45a1da9c64df4b62a8d1cb4599ed0","883fb6beb2dd415a875165f37173d099","5dc7d43a11f746bebf76176b1e0aed6c","f450a67668334be0bfc634b1a00c6053","b3bc7855aed44dfc82cf6fcadc37eeeb","ba82787f6aa3406db7a33dbc32259c0b","616cd9a9be02416db00bb56cf6eafa42","fbce241a80ed4c95a6aa17b5df7fc3c3","15bfd9a6e21a40fe857553bb49c29d8f","96a1b966a732441487e97def4a07ed9b","1e57efbcafd641fda517efd0eff4fcaf","c42eef1278d44df0942dce2479d5b531","f366bd736bed4f05952b94c276e0e992","7040058262404c66bc6917d5117c4960","96caa9d774f04604be6e347ed9e230fc","93967097184045f986f5002663e6351e","22e36b0d74374a4297ea890c47e74405","3a361d77921843e48c983abddd71e095","82d2e14a3d144f99be07911d1aab7090","b7890c66b68e445593adf36c6a83549f","f868237c94d54e20b7593a567b8131dd","3900ba35354b4239b5ab233d9e653dcf","9a51378f5d174b6c9558479b3bf64a0f","f4b6e076c3f04f94a94465826fe4fe01","2cf28e168f804fed9072dc9dfc06ab0c","4158f40f67964a158e1557e129722672","4f3835724dba407899b601d6cc039b6b","6d0cebd30099424a86b508546b565e9c","d96b3fcf3cf043c2bd7bcb0bcfa920a4","d362ff5ae594474ea6815d29a0034ec3","d7430f14a2084698a3ad7ef35b49f2bc","52538d913bd94f1f97843dad45164e32","ca8e1060d4464b2aa517b50c22c1ca41","005f827fdd5c4217a99709113c57a332","49a2fe799ea24b2b808f0007c0c03251","301ee82e3c884177b69a0f7effbf5cf7","8cccfa9bce4e4fbeb5ff026834739da0","e1f701e21259444c8e5aad1089419eb0","6d3d487521714c64b39302fede1f84c9","45c2ea6f547e4a7e8aca813688194456","d63a8256a9204af789a20e9c2d2a36bf","658e45df03f745a4863400f994fba5c1","eb612a7909744d4cab11e1605041550b","46ca6ab6f2a04c6eb8f9a2a9c65d8ae0","ec85070387934860b00a9c8dc805fc11","fb40b0fba4ba4f94b6eec2ac7c0df182","7eaeaa055e7d4d509fb609f9fba497d0","a2fdf853b16c48c4aa0e8725ffc6b130","f3f6b5f229024caeb0003898187a6efa","cf836ee0fcdd42d49f4cdeeafa3e3050","3aea2453b5e445eb85a10df16541f35e","55f08a97fa3d4c2693b42c07c8b5a218","839b17257ef346b3b8ae5afa9f8b9bda","3026791b06114e13ae27af20667e252e","24de7ba49cac41378b32fd4dc6dbb301","87bac54cde4843cf80bc0b28ee6a5631","c6dc082b2c904bf1ae1222226cec5793","bfc6aee65c934919adc711403e09575a","8bd1aa1c71b347699500b590f3aacc2f","195a3c59763a4a229054ce327e7d13df","d5008330ad1344369c687af81ee98034","d665047dec7f482e818a31df6d31c1e4","d4b119f74c1241bf866a4429a585c6df","15c1a7b51a3345368542d6b72bc3811e","8930cc34626c4aaeb59b91fd0c287b4c","7213d6a88ca54a89af4d3ac3a36456ab","de5c719d39ec465487a783ba3b40c36a","351aeb359f5b465ba294e0399a1cd951","6ff2ad315edb481ca67a6ad246e77f33","6ed5718e3f7c4b2384bd398f3e06c3bd","137a51113e1c4a32af800ecbf1b5c31e","c6eb2c43ba9a4c1a99d7a58a7a980d04","39b1b28db89840d1867772332ff4a4ad","fe08bf8457e643dfa2e243783e8da86e","d62485e8fe364432ab70dfc2136d5dc1","b7d8fe44f6224c66927f31d405a80b91","d0f8a283f46b41e88f4318d7e9e3ab70","d324822572d544f4beed5483dea0c137","cfb74b71b86e44da94e9c5197dc08b0a","ed8c5e33578c4b06bcfb7365df17f62d","6dda7c5a21764bf0b78f0e009ec3bb94","c850f86868e94938ae5dd25688a1700a","6c8c17ff01994280aa237372a7aa4953","365beb9e232a474da76d0eb883a1a509","06f34bd69b074d96a4949917c0a1f4f9","45821ba090af4d689afae1b8aa4b57d5","11b87603c63f4ed388bcec564dcecc66","0c23a2f6edb549e891af94b1adc8453f","5b0847ac217b4ada956614b2ea4eac91","4641c1acdc774fadbeca3d023747e505","8d682655d4e747c9ae3dc9c86792f195","67765f027dcc457a872668a9fa5c76ff","fb9784b0f9034b3db9abfa9f6388edaf","98e16956c4d24576910910a2d19a1198","c3cba52a7db24652ab4a72294cddaea4","ae7d9c36b78244b0a88d5868bfeba490","84698d9d279344778131d0d68313b36c","bc5363e13f3e4e4fb3302515266df7b0","781895aa6b414034b278afedf1c863f0","38b6c5dbd45840f18bf96725c3fc8ed2","7d42bba59298452ab228fb168e3677ca","c80c3730f0dd43509aaf124278e34634","5e6001c0284f461994f7ec76af7c7642","393c31d0548d44b99ea55bd31e0a0f05","56fbc8457179456ca5bd33b942c129a1","ef3a941f24b34f35b044ff9032e29885","7472333ae2eb42ae8a6380502e1fa568","7b19c19671cb4687821e92a67deef92f","8b8a10ed16c44bcb9c57009039662d73","8f43f8805cd54779893bad54961f2318","eec452a3b3f847a0844dfcc7a9c32044","c9af9d3b51244a6c96b06af2509dd39d","60890527ad3f4f7a9500e3bd79715f11","f4d1ea2470474602ad4b6b8f8db72638","a2c8fc615e494907b963bac09249f2f0","6a441421035f4b9590fea7186acbd92b","604eda3164ee456fa04798c7449c9855","70fdc6f6b5e24ddea16f5e6a0dd624d8","f3ad867ea4b842fe91ab572d15edd6d9","2d3bb7fc2d17483bb062fccf88c577e7","cd3ea5a03bc34036b86fe1b4dba2eeb4","727af5b3883440dca753c7486d252d97","2db243a4fa3d422f9c5445cf6af6fea5","5187a7b6586c43ffa7334bf060dc1761","0a93a59da7504d3380efa62c17e7de7f","c24bc877fe374ab2b30326a88eec45af","001650a5ce414da0a5fec1b045a538f9","3a606736a4824e0db2c5dd5e471c954a","ba51cddde50d42daa7241e8f25fb1f7c","b355c9ae2dc54f978702b633b5f86195","9a0487614b3144d18b5bae050209990f","41bfdb674472434887df58987dbf5b6a","63d05617a45740e099b4e74cf103bba9","c70ede7bdeac4f18b04e3e974c69c605","bd041a7f402c4551be5d354a709a8960","f620c1cea0f44994a65a6edcb4678317","86c77e53a09244bd92aa2c0b481177c8","fadc2b2349824394bd6f0b3aafb2af8f","d19cea5a218f4ab09af64b8e544a59b5","30f5ab997ae94ddf93dbdbb119859099","bb127c246bea4dc28f860536812c0341","443e9b5d3d0841fe981e5e0d1fa30f86","6556310a79544a6d86a709f3b1a988e6","1d64f3a1f0ce446cac0436a836e8e768","43907c5a148e4a93a579fd0f06d0dd86","67d75125c1a649c6abffd1e4042f3b36","2b3d30416ecd4ef4be770526635e1f4b","0550c2eba01e41bc88dc87930d245f76","22c4dbb28af74ffc92d063e548e2047f","a3465a53e56a4726b04d64b106de7d14","107c96ae31974dd092677a8fc7480b5b","f2214192e62c4c0189b5c5b1e8a0e8cb","c6236fa8d02f4588982b671b0223c3be","18538b4e597d4287a229b920e3420e34","7910b26c70e741b298359934d3645575","4be05a5f6de542a78f397c39726ee105","6663a01c91bb49c6b526e73d0a72cb1c","3e9340db94c9456fa3058c05837af0c1","b6529766373b4456b0ef3c70c39cf1a2","a539d0718bd0426f9afb24742f970981","46b13bb87f6848bfb887ab75bee963d4","fc21aeaa24d34c69af6e8b9d3f61fc23","a7904b74e5a94a9383fe98439cdfe458","de1277e3bd69467c93a1f1b30d06623e","2eddcb857bd44d3a9c98fa0965e1a7a7","e9053c547ace4507a33d816681a36bad","dc5d8d293e554f7b8c7860cfd7d6addb","90c7151ce643489bb8682413d5f9862f","0938a1b83f7942ad807be8e5fcb8351b","1e344241b5334032a90fb6722fbe438b","7dadb91a174142af9a98e64d9df6142b","177678b00b6a4f358e1f741ace9235c1","86967d3715024cedb3624a84d05e67be","d7af1093689f4b209963c4a74e6408f1","ff35e6813ee6416b8682dafdfff312c9","823e705ab04d4558890f466bfcce36fb","ed539569edad423e9dc625a80a42848b","2bbed3e6d17a4446ad5d459168c2f3fb","3ed437fb6ca2464aa09aebe80ce555fe","5c6d94883fb44cbe877c78d68d35fad4","54970e3a30a448988fca43f5dc8c6642","396db771e14d457188b126e3e21ef3ab","4d4da668804048458d4c0615cf0b82fe","4d134fbab06946afbecb6262dec84d3f","e890cc58752c4b18a0f2653f7466120a","758b398a690448239de89aaa57ab7111","2b96644b9548492ba4d4e4c6e4061893","8512a35411c643398f0e37c57c29484c","fc22e3d2cbfe4a7f8c5831bb1cfe36cf","91ca476acfd54b29a00d70bbe0f50e98"]},"id":"LxxAVBxGI4Yb","outputId":"c0a1b6d4-6a8c-46f4-8562-c30d838ddc2e","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3423a9b6255b4e63aed7b2f9501e35fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00000-of-00011.parquet:   0%|          | 0.00/451M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f450a67668334be0bfc634b1a00c6053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00001-of-00011.parquet:   0%|          | 0.00/452M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96caa9d774f04604be6e347ed9e230fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00002-of-00011.parquet:   0%|          | 0.00/456M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4158f40f67964a158e1557e129722672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00003-of-00011.parquet:   0%|          | 0.00/449M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cccfa9bce4e4fbeb5ff026834739da0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00004-of-00011.parquet:   0%|          | 0.00/455M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2fdf853b16c48c4aa0e8725ffc6b130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00005-of-00011.parquet:   0%|          | 0.00/453M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd1aa1c71b347699500b590f3aacc2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00006-of-00011.parquet:   0%|          | 0.00/452M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed5718e3f7c4b2384bd398f3e06c3bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00007-of-00011.parquet:   0%|          | 0.00/457M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dda7c5a21764bf0b78f0e009ec3bb94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00008-of-00011.parquet:   0%|          | 0.00/453M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67765f027dcc457a872668a9fa5c76ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00009-of-00011.parquet:   0%|          | 0.00/452M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e6001c0284f461994f7ec76af7c7642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00010-of-00011.parquet:   0%|          | 0.00/450M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d1ea2470474602ad4b6b8f8db72638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/184M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a93a59da7504d3380efa62c17e7de7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test-00000-of-00002.parquet:   0%|          | 0.00/316M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f620c1cea0f44994a65a6edcb4678317"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test-00001-of-00002.parquet:   0%|          | 0.00/312M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3d30416ecd4ef4be770526635e1f4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/54087 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e9340db94c9456fa3058c05837af0c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0938a1b83f7942ad807be8e5fcb8351b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/6854 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c6d94883fb44cbe877c78d68d35fad4"}},"metadata":{}}]},{"cell_type":"code","source":["from tqdm import tqdm\n","import os\n","from datasets import load_dataset\n","\n","# Создание директории images внутри /content\n","image_dir = \"images\"  #  Относительный путь!\n","os.makedirs(image_dir, exist_ok=True)\n","\n","# Извлечение и сохранение ТОЛЬКО тестовых изображений\n","image_paths = []\n","for i, example in enumerate(tqdm(ds['test'], desc=\"Saving test images\")):\n","    image = example['image']\n","    image_path = os.path.join(image_dir, f\"test_image_{i}.jpg\")  # Относительный путь\n","    image.save(image_path)\n","    image_paths.append(image_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KbQpn9qYKYj9","outputId":"3fd937e1-a192-4143-ecc0-efd70eb13208"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving test images: 100%|██████████| 6854/6854 [00:36<00:00, 185.54it/s]\n"]}]},{"cell_type":"code","source":["from pathlib import Path\n","import gdown\n","import zipfile\n","import os\n","import shutil\n","\n","# ID папки с моделями\n","models_folder_id = \"1DEhVyVfpQ3_ppcArNYyEwtj6Z9RtVeAq\"\n","models_output_dir = Path(\"/content/models\")\n","models_output_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Скачиваем архив с моделями\n","url = f'https://drive.google.com/drive/folders/{models_folder_id}?usp=sharing'\n","gdown.download_folder(url, output=str(models_output_dir), quiet=True, use_cookies=False)\n","\n","# Ищем zip-архивы и распаковываем их\n","zip_archives = list(models_output_dir.glob(\"*.zip\"))\n","for zip_archive_path in zip_archives:\n","    # Распаковываем архив во временную папку\n","    temp_extract_dir = models_output_dir / \"temp_extract\"\n","    with zipfile.ZipFile(zip_archive_path, 'r') as zip_ref:\n","        zip_ref.extractall(temp_extract_dir)\n","\n","    # Перемещаем содержимое из content (если есть) в models_output_dir\n","    content_dir = temp_extract_dir / \"content\"\n","    if content_dir.exists():\n","        for item in content_dir.iterdir():\n","            final_path = models_output_dir / item.name\n","            if final_path.exists():\n","                if final_path.is_dir():\n","                    shutil.rmtree(final_path)  # Удаляем, если директория\n","                else:\n","                    os.remove(final_path) # Удаляем, если файл\n","            shutil.move(str(item), str(models_output_dir))\n","\n","    # Удаляем временную папку\n","    shutil.rmtree(temp_extract_dir)\n","    # Удаляем сам архив\n","    os.remove(zip_archive_path)"],"metadata":{"id":"nAM0RUH5pz2X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Собираем пути к моделям\n","model_paths = {}\n","\n","# PyTorch\n","pytorch_model = next(models_output_dir.glob(\"*.pt\"), None)\n","if pytorch_model:\n","    model_paths[\"PyTorch\"] = str(pytorch_model)\n","\n","# ONNX\n","onnx_model = next(models_output_dir.glob(\"*.onnx\"), None)\n","if onnx_model:\n","    model_paths[\"ONNX\"] = str(onnx_model)\n","\n","# TorchScript\n","ts_model = next(models_output_dir.glob(\"*.torchscript\"), None)\n","if ts_model:\n","    model_paths[\"TorchScript\"] = str(ts_model)\n","\n","# TensorRT\n","tf_model = next(models_output_dir.glob(\"*.engine\"), None)\n","if tf_model:\n","  model_paths[\"TensorRT\"] = str(tf_model)\n","\n","print(model_paths)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8pSe1JSXpzwV","outputId":"4ae402d0-0a4f-4d3f-a66f-10f946d1cceb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'PyTorch': '/content/models/yolo11m-obb.pt', 'ONNX': '/content/models/yolo11m-obb.onnx', 'TorchScript': '/content/models/yolo11m-obb.torchscript', 'TensorRT': '/content/models/yolo11m-obb.engine'}\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","loaded_models = {}\n","for format, model_path in model_paths.items():\n","    try:\n","        loaded_models[format] = YOLO(model_path, task='obb')\n","        print(f\"Model loaded from {model_path} for format {format}\")\n","    except Exception as e:\n","        print(f\"Error loading model for {format}: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAQMG60wqfwh","outputId":"55bec7f9-2a80-4215-b427-b408dd1f6fca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","Model loaded from /content/models/yolo11m-obb.pt for library PyTorch\n","Model loaded from /content/models/yolo11m-obb.onnx for library ONNX\n","Model loaded from /content/models/yolo11m-obb.torchscript for library TorchScript\n","Model loaded from /content/models/yolo11m-obb.engine for library TensorRT\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","from ultralytics import YOLO\n","import psutil\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","\n","def get_predictions(results) -> dict:\n","    \"\"\"\n","    Преобразует результаты работы YOLO в словарь (без изменений).\n","    \"\"\"\n","    image_height, image_width = results.orig_shape\n","    names = results.names\n","    obboxes = results.obb.data.tolist()\n","    objects = []\n","    for obj in obboxes:\n","        X, Y, W, H, R = int(obj[0]), int(obj[1]), int(obj[2]), int(obj[3]), int(obj[4])\n","        confidence = obj[5]\n","        label = int(obj[6])\n","        keys = ['confidence', 'name', 'X', 'Y', 'Width', 'Heigth', 'Rotation']\n","        result_dict = {key: val for key, val in zip(keys, [confidence, names[label], X, Y, W, H, R])}\n","        objects.append(result_dict)\n","    return {\n","        \"height\": int(image_height),\n","        \"objects\": objects,\n","        \"width\": int(image_width),\n","    }\n","\n","\n","def get_results(image_paths, format, model):\n","    \"\"\"\n","    Инференс YOLO, измерение общего времени обработки, пиковой памяти и отрисовка.\n","    \"\"\"\n","    results_list = []\n","    total_inference_time = 0.0  # Общее время (препроцессинг + инференс + постпроцессинг)\n","    peak_memory = 0  # Пиковое использование памяти\n","\n","    for image_path in image_paths:\n","        if not os.path.exists(image_path):\n","            print(f\"Image not found: {image_path}\")\n","            continue\n","\n","        try:\n","            img = cv2.imread(image_path)\n","            if img is None:\n","                print(f\"Could not read image: {image_path}\")\n","                continue\n","\n","            # Инференс\n","            results = model(img, imgsz=(1024, 1024))[0]\n","\n","            # Получаем текущее использование памяти и обновляем пиковое значение\n","            current_memory = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)  # in MB\n","            peak_memory = max(peak_memory, current_memory)\n","\n","            # Суммируем времена из results.speed (переводим из мс в секунды)\n","            total_inference_time += (results.speed['preprocess'] +\n","                                      results.speed['inference'] +\n","                                      results.speed['postprocess']) / 1000.0\n","\n","            # Получение и добавление предсказаний\n","            predictions = get_predictions(results)\n","            results_list.append({\n","                \"image_path\": image_path,\n","                \"predictions\": predictions,\n","                \"model_speed\": results.speed,  # Скорость инференса для этого изображения\n","            })\n","\n","            # Отрисовка результатов\n","            # plot_results(results, img, image_path)\n","\n","        except Exception as e:\n","            print(f\"Error processing image {image_path}: {e}\")\n","            continue\n","\n","    return {\n","        \"format\": format,\n","        \"results\": results_list,\n","        \"total_inference_time\": total_inference_time,\n","        \"peak_memory_MB\": peak_memory,\n","    }\n","\n","\n","def plot_results(results, img, image_path):\n","    \"\"\"Отрисовывает результаты на изображении и показывает/сохраняет его.\"\"\"\n","\n","    #  Используем метод plot() из Results, предоставленный ultralytics\n","    img_with_boxes = results.plot()\n","\n","    # Отображение в Colab (используем matplotlib)\n","    plt.figure(figsize=(10, 10))  # Устанавливаем размер изображения\n","    plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))  # Преобразуем BGR в RGB для matplotlib\n","    plt.title(f\"Inference Result: {image_path}\")\n","    plt.axis('off')  # Убираем оси\n","    plt.show()"],"metadata":{"id":"-wcMgfGUpztT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iterations = 1\n","final_results = [] # Содержит агрегированные данные\n","all_raw_results = {}  # Словарь для хранения \"сырых\" результатов по библиотекам и итерациям\n","\n","for iteration in range(iterations):\n","    data = []\n","    for format, model in loaded_models.items():\n","        results = get_results(image_paths, format, model)\n","\n","        # Сохраняем \"сырые\" результаты\n","        if format not in all_raw_results:\n","            all_raw_results[format] = {}\n","        if iteration not in all_raw_results[format]:\n","            all_raw_results[format][iteration] = []\n","        all_raw_results[format][iteration].append(results['results']) #Сохраняем results['results']\n","\n","\n","        avg_preprocess_time = sum([r['model_speed']['preprocess'] for r in results['results']]) / len(results['results'])\n","        avg_inference_time = sum([r['model_speed']['inference'] for r in results['results']]) / len(results['results'])\n","        avg_postprocess_time = sum([r['model_speed']['postprocess'] for r in results['results']]) / len(results['results'])\n","\n","        data.append({\n","            \"Format\": results['format'],\n","            \"Total Inference Time (s)\": results['total_inference_time'],\n","            \"Peak Memory Usage (MB)\": results['peak_memory_MB'],\n","            \"Avg. Preprocess Time (ms)\": avg_preprocess_time,\n","            \"Avg. Inference Time (ms)\": avg_inference_time,\n","            \"Avg. Postprocess Time (ms)\": avg_postprocess_time,\n","        })\n","    final_results.append(pd.DataFrame(data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Kiq6XuRRqqEG","outputId":"1b358123-a42b-44a9-95dc-b888cd09378b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n","0: 1024x1024 43.9ms\n","Speed: 9.0ms preprocess, 43.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.4ms preprocess, 45.4ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.5ms preprocess, 45.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.6ms preprocess, 45.2ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 11.9ms preprocess, 44.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.3ms\n","Speed: 10.0ms preprocess, 45.3ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 11.5ms preprocess, 46.0ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 8.5ms preprocess, 44.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 11.0ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.2ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.3ms preprocess, 45.9ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.0ms preprocess, 45.2ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 10.4ms preprocess, 45.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.6ms preprocess, 45.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.0ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 11.0ms preprocess, 44.7ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.0ms preprocess, 46.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 42.5ms\n","Speed: 14.2ms preprocess, 42.5ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 13.1ms preprocess, 45.1ms inference, 10.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.7ms preprocess, 44.4ms inference, 5.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 11.8ms preprocess, 43.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.4ms preprocess, 45.8ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 12.6ms preprocess, 45.8ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.4ms preprocess, 45.4ms inference, 7.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.1ms\n","Speed: 13.5ms preprocess, 43.1ms inference, 5.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 12.3ms preprocess, 45.7ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.9ms preprocess, 45.8ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 11.6ms preprocess, 44.4ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.5ms preprocess, 46.1ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.3ms preprocess, 44.7ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.6ms preprocess, 45.1ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 9.8ms preprocess, 45.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 11.3ms preprocess, 46.3ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.8ms preprocess, 46.0ms inference, 7.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 12.6ms preprocess, 43.9ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.2ms preprocess, 45.6ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.2ms\n","Speed: 9.9ms preprocess, 45.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 11.1ms preprocess, 45.9ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.7ms\n","Speed: 13.5ms preprocess, 43.7ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.8ms preprocess, 46.1ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.2ms preprocess, 45.3ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.6ms preprocess, 44.9ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.5ms preprocess, 45.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 10.1ms preprocess, 47.0ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.0ms preprocess, 44.9ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.0ms preprocess, 44.9ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 10.4ms preprocess, 44.6ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.0ms preprocess, 45.5ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.0ms preprocess, 45.7ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.2ms\n","Speed: 10.5ms preprocess, 43.2ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.4ms preprocess, 46.5ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.7ms preprocess, 46.0ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.3ms preprocess, 45.6ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.4ms preprocess, 45.3ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.6ms\n","Speed: 9.3ms preprocess, 44.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.6ms preprocess, 45.7ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.9ms preprocess, 45.7ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.4ms\n","Speed: 11.8ms preprocess, 42.4ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.2ms preprocess, 45.5ms inference, 9.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.8ms\n","Speed: 11.6ms preprocess, 42.8ms inference, 9.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 14.1ms preprocess, 43.7ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 13.2ms preprocess, 44.3ms inference, 6.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 11.8ms preprocess, 44.0ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.6ms\n","Speed: 13.7ms preprocess, 42.6ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 12.5ms preprocess, 43.8ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.8ms preprocess, 45.8ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.5ms\n","Speed: 14.6ms preprocess, 42.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.3ms preprocess, 45.4ms inference, 5.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 12.2ms preprocess, 43.2ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.0ms\n","Speed: 12.5ms preprocess, 43.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.3ms\n","Speed: 9.7ms preprocess, 47.3ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 9.1ms preprocess, 47.0ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.6ms\n","Speed: 16.2ms preprocess, 42.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.0ms preprocess, 45.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 8.8ms preprocess, 47.0ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.1ms preprocess, 46.3ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.3ms\n","Speed: 14.7ms preprocess, 44.3ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 9.8ms preprocess, 46.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.2ms preprocess, 45.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 9.0ms preprocess, 44.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.2ms preprocess, 45.8ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.7ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.4ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.6ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.2ms preprocess, 45.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.4ms preprocess, 46.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.1ms preprocess, 45.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.2ms preprocess, 45.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 8.4ms preprocess, 46.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.5ms preprocess, 45.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.5ms preprocess, 45.7ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.9ms preprocess, 45.2ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.9ms preprocess, 45.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.9ms\n","Speed: 9.2ms preprocess, 44.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.0ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 9.7ms preprocess, 45.4ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 12.3ms preprocess, 44.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 9.7ms preprocess, 47.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 10.0ms preprocess, 45.7ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.8ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.1ms preprocess, 45.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.5ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.3ms preprocess, 44.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 9.3ms preprocess, 46.9ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 12.6ms preprocess, 44.4ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.7ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.8ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.0ms preprocess, 45.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.5ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 10.0ms preprocess, 44.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.5ms\n","Speed: 9.7ms preprocess, 43.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.2ms preprocess, 45.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.2ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.5ms preprocess, 45.0ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.2ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.0ms preprocess, 45.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.6ms preprocess, 45.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.2ms preprocess, 46.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.9ms preprocess, 45.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.5ms preprocess, 44.7ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 14.8ms preprocess, 44.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.6ms preprocess, 45.2ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.6ms\n","Speed: 10.2ms preprocess, 43.6ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.9ms preprocess, 45.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 11.2ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 8.5ms preprocess, 45.0ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.5ms\n","Speed: 10.4ms preprocess, 44.5ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 9.9ms preprocess, 47.0ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.7ms preprocess, 45.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.2ms preprocess, 45.5ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.9ms preprocess, 44.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 11.7ms preprocess, 44.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 10.8ms preprocess, 46.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.9ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.2ms preprocess, 45.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 10.7ms preprocess, 44.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.6ms preprocess, 45.8ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.0ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 9.9ms preprocess, 45.7ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.7ms preprocess, 46.2ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 13.9ms preprocess, 44.1ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 11.0ms preprocess, 44.8ms inference, 5.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.5ms\n","Speed: 11.9ms preprocess, 43.5ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.7ms preprocess, 45.0ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.5ms\n","Speed: 11.2ms preprocess, 44.5ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.2ms\n","Speed: 9.6ms preprocess, 47.2ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 11.0ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.3ms preprocess, 46.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 10.2ms preprocess, 44.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 10.6ms preprocess, 46.9ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 14.2ms preprocess, 43.2ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 11.0ms preprocess, 44.4ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.2ms\n","Speed: 10.9ms preprocess, 47.2ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 10.2ms preprocess, 47.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 12.1ms preprocess, 43.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.6ms preprocess, 44.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.9ms preprocess, 45.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.9ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 8.8ms preprocess, 46.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.7ms preprocess, 45.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.2ms preprocess, 44.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.3ms\n","Speed: 9.8ms preprocess, 46.3ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.5ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.5ms preprocess, 46.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 9.9ms preprocess, 44.2ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 9.6ms preprocess, 43.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 11.0ms preprocess, 45.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.1ms\n","Speed: 11.5ms preprocess, 46.1ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.8ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.5ms preprocess, 44.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.7ms preprocess, 44.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.6ms preprocess, 45.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.6ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.6ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.5ms\n","Speed: 10.2ms preprocess, 43.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 8.8ms preprocess, 44.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 7.8ms preprocess, 47.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 8.3ms preprocess, 45.3ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.9ms preprocess, 45.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.8ms preprocess, 45.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.5ms preprocess, 44.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 9.6ms preprocess, 44.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.9ms preprocess, 45.6ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 8.6ms preprocess, 46.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.5ms\n","Speed: 9.3ms preprocess, 46.5ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.7ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.8ms\n","Speed: 8.9ms preprocess, 44.8ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.8ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 11.9ms preprocess, 44.7ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 9.4ms preprocess, 46.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 10.0ms preprocess, 45.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.7ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.5ms preprocess, 45.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 10.2ms preprocess, 46.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.6ms preprocess, 46.4ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 11.1ms preprocess, 45.6ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.0ms\n","Speed: 14.7ms preprocess, 43.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 48.0ms\n","Speed: 10.1ms preprocess, 48.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.4ms preprocess, 45.3ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.1ms preprocess, 46.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 10.0ms preprocess, 44.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.8ms preprocess, 45.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.5ms preprocess, 45.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.2ms preprocess, 45.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.2ms preprocess, 45.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.8ms preprocess, 44.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.6ms preprocess, 46.5ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.9ms\n","Speed: 9.8ms preprocess, 44.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 9.2ms preprocess, 46.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.5ms preprocess, 46.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.8ms preprocess, 46.5ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.8ms\n","Speed: 14.6ms preprocess, 42.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.8ms preprocess, 46.2ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 10.1ms preprocess, 46.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.5ms preprocess, 44.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.2ms preprocess, 45.0ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 10.8ms preprocess, 44.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.0ms preprocess, 46.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.6ms preprocess, 46.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 13.3ms preprocess, 44.3ms inference, 7.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 11.3ms preprocess, 45.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.3ms preprocess, 45.4ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.4ms preprocess, 45.9ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.8ms preprocess, 45.3ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.5ms\n","Speed: 11.6ms preprocess, 44.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.9ms preprocess, 46.2ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.4ms\n","Speed: 15.2ms preprocess, 43.4ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.1ms preprocess, 45.4ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.1ms preprocess, 45.7ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.3ms\n","Speed: 10.9ms preprocess, 43.3ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.5ms preprocess, 45.2ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.7ms preprocess, 46.4ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.3ms preprocess, 46.0ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.5ms preprocess, 45.7ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.8ms preprocess, 45.8ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.8ms preprocess, 45.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.5ms\n","Speed: 9.3ms preprocess, 47.5ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 11.6ms preprocess, 45.6ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.2ms preprocess, 46.3ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.6ms preprocess, 45.1ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.8ms\n","Speed: 12.5ms preprocess, 42.8ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 11.1ms preprocess, 45.2ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.3ms preprocess, 44.3ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 10.3ms preprocess, 44.1ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 11.4ms preprocess, 45.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.2ms preprocess, 46.3ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.8ms preprocess, 45.3ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 10.7ms preprocess, 43.7ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.2ms preprocess, 45.2ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.9ms preprocess, 45.2ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.2ms preprocess, 45.4ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.2ms\n","Speed: 10.8ms preprocess, 43.2ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 48.8ms\n","Speed: 9.1ms preprocess, 48.8ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 11.4ms preprocess, 44.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 12.7ms preprocess, 45.1ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.2ms preprocess, 46.2ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.1ms preprocess, 45.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 13.3ms preprocess, 44.9ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.5ms preprocess, 45.4ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.6ms\n","Speed: 10.4ms preprocess, 44.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 14.4ms preprocess, 44.1ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.4ms preprocess, 45.7ms inference, 6.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.3ms preprocess, 45.0ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.8ms\n","Speed: 10.1ms preprocess, 42.8ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.5ms preprocess, 45.0ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 14.1ms preprocess, 44.7ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 49.2ms\n","Speed: 9.6ms preprocess, 49.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.4ms\n","Speed: 8.9ms preprocess, 47.4ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.5ms preprocess, 45.3ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 13.4ms preprocess, 44.0ms inference, 6.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 15.1ms preprocess, 43.2ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 9.8ms preprocess, 43.2ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.3ms preprocess, 46.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 8.4ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.5ms preprocess, 45.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 9.1ms preprocess, 44.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 10.4ms preprocess, 45.7ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.5ms\n","Speed: 9.6ms preprocess, 47.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.7ms preprocess, 46.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.7ms\n","Speed: 10.9ms preprocess, 46.7ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.1ms\n","Speed: 9.4ms preprocess, 47.1ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.7ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 8.9ms preprocess, 44.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 8.8ms preprocess, 45.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.4ms preprocess, 45.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.5ms preprocess, 46.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.6ms\n","Speed: 10.8ms preprocess, 44.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 10.6ms preprocess, 45.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.6ms preprocess, 45.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 10.3ms preprocess, 43.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 9.9ms preprocess, 45.8ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 9.0ms preprocess, 46.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.5ms preprocess, 45.1ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.7ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.5ms preprocess, 45.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.1ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.4ms preprocess, 46.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.7ms preprocess, 46.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.4ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.5ms preprocess, 45.8ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.1ms preprocess, 45.6ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.1ms preprocess, 45.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 10.2ms preprocess, 44.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.2ms preprocess, 45.5ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 11.3ms preprocess, 45.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.6ms preprocess, 46.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.5ms preprocess, 45.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.2ms preprocess, 45.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.9ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.3ms preprocess, 46.0ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.2ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 9.4ms preprocess, 43.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.9ms\n","Speed: 9.8ms preprocess, 46.9ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.4ms preprocess, 45.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.0ms preprocess, 46.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.2ms\n","Speed: 12.7ms preprocess, 45.2ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 13.0ms preprocess, 44.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.2ms preprocess, 45.2ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.6ms\n","Speed: 12.6ms preprocess, 44.6ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.5ms\n","Speed: 9.1ms preprocess, 47.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 11.5ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.6ms preprocess, 45.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.3ms preprocess, 45.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.8ms preprocess, 46.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 10.6ms preprocess, 45.8ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.8ms preprocess, 46.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.9ms preprocess, 45.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.1ms preprocess, 46.0ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.4ms preprocess, 46.2ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 11.2ms preprocess, 45.8ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 10.4ms preprocess, 44.1ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 13.2ms preprocess, 45.2ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.8ms preprocess, 46.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 10.9ms preprocess, 45.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.2ms preprocess, 44.9ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 8.8ms preprocess, 45.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.4ms preprocess, 45.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.0ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.8ms preprocess, 45.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.0ms\n","Speed: 10.0ms preprocess, 45.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 10.0ms preprocess, 46.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.9ms preprocess, 44.5ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.6ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 11.6ms preprocess, 45.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.2ms preprocess, 44.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.7ms preprocess, 44.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.1ms\n","Speed: 10.6ms preprocess, 45.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 9.4ms preprocess, 46.4ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.1ms preprocess, 45.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.6ms preprocess, 45.2ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 10.5ms preprocess, 43.9ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.6ms preprocess, 45.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.4ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 10.6ms preprocess, 46.4ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.9ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.6ms preprocess, 45.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.2ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.1ms\n","Speed: 9.6ms preprocess, 46.1ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.8ms preprocess, 46.7ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 9.6ms preprocess, 45.5ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.6ms\n","Speed: 9.4ms preprocess, 47.6ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.0ms preprocess, 46.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.1ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.8ms preprocess, 45.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.4ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.5ms preprocess, 45.4ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.8ms preprocess, 45.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 11.4ms preprocess, 46.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.4ms preprocess, 46.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.6ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.0ms preprocess, 46.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.4ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.5ms preprocess, 46.2ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.0ms preprocess, 45.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.5ms preprocess, 45.5ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.5ms preprocess, 44.4ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 11.7ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 11.2ms preprocess, 45.2ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.2ms\n","Speed: 10.9ms preprocess, 44.2ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.4ms preprocess, 44.9ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.7ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.0ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 8.6ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 8.8ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.0ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.5ms preprocess, 46.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.6ms preprocess, 46.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.5ms preprocess, 46.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 9.2ms preprocess, 46.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.2ms\n","Speed: 9.7ms preprocess, 45.2ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 15.8ms preprocess, 43.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.6ms\n","Speed: 9.2ms preprocess, 47.6ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 13.1ms preprocess, 44.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.1ms\n","Speed: 9.3ms preprocess, 45.1ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.9ms\n","Speed: 9.2ms preprocess, 47.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.7ms preprocess, 45.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.2ms preprocess, 46.2ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.4ms preprocess, 46.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.6ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.7ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.5ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.0ms preprocess, 46.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.1ms preprocess, 46.5ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.4ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.7ms preprocess, 45.8ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 12.0ms preprocess, 44.2ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 10.1ms preprocess, 44.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 10.1ms preprocess, 46.6ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.6ms\n","Speed: 15.3ms preprocess, 42.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.7ms preprocess, 45.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.9ms preprocess, 45.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.6ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.6ms preprocess, 46.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 11.8ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.2ms preprocess, 45.5ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.1ms preprocess, 46.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.5ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.9ms preprocess, 46.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.6ms preprocess, 46.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.0ms preprocess, 46.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 8.9ms preprocess, 45.0ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 13.4ms preprocess, 45.9ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 13.8ms preprocess, 43.7ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.1ms\n","Speed: 12.7ms preprocess, 45.1ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.3ms preprocess, 45.2ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 13.8ms preprocess, 44.5ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 14.1ms preprocess, 45.0ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.9ms preprocess, 45.3ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 13.5ms preprocess, 44.5ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 12.3ms preprocess, 46.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.4ms\n","Speed: 9.9ms preprocess, 44.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 11.3ms preprocess, 46.0ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.9ms preprocess, 45.1ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.6ms preprocess, 45.7ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 8.9ms preprocess, 45.4ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.0ms preprocess, 46.4ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 48.4ms\n","Speed: 9.0ms preprocess, 48.4ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 12.3ms preprocess, 43.7ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 11.2ms preprocess, 45.2ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.6ms preprocess, 45.0ms inference, 6.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 8.9ms preprocess, 44.7ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.4ms preprocess, 45.5ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.8ms\n","Speed: 9.0ms preprocess, 46.8ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 9.1ms preprocess, 47.0ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.6ms preprocess, 45.0ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.2ms preprocess, 45.9ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 9.3ms preprocess, 45.5ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 48.1ms\n","Speed: 9.1ms preprocess, 48.1ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.9ms\n","Speed: 8.9ms preprocess, 47.9ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 8.8ms preprocess, 45.7ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.9ms preprocess, 45.9ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.2ms\n","Speed: 15.1ms preprocess, 42.2ms inference, 5.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.4ms\n","Speed: 12.4ms preprocess, 47.4ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.4ms\n","Speed: 13.5ms preprocess, 42.4ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.3ms\n","Speed: 10.5ms preprocess, 46.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.5ms preprocess, 45.1ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.8ms\n","Speed: 14.5ms preprocess, 42.8ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.8ms\n","Speed: 11.5ms preprocess, 44.8ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.4ms preprocess, 46.0ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.6ms preprocess, 44.9ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 9.0ms preprocess, 45.5ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.5ms\n","Speed: 9.7ms preprocess, 47.5ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.8ms preprocess, 45.8ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 12.2ms preprocess, 44.2ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 15.3ms preprocess, 43.8ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 11.0ms preprocess, 46.6ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 9.8ms preprocess, 44.0ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 16.8ms preprocess, 44.0ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 14.1ms preprocess, 44.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 9.6ms preprocess, 45.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.1ms\n","Speed: 9.4ms preprocess, 47.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.3ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.3ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 11.2ms preprocess, 44.8ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 11.8ms preprocess, 44.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.9ms preprocess, 45.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.8ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.6ms preprocess, 45.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.2ms preprocess, 45.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.5ms preprocess, 44.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.7ms preprocess, 46.5ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.2ms preprocess, 46.3ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 9.8ms preprocess, 44.6ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.2ms preprocess, 45.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 8.5ms preprocess, 46.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.3ms preprocess, 45.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.2ms preprocess, 45.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.1ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.1ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.3ms preprocess, 46.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 9.2ms preprocess, 44.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.1ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.4ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.8ms preprocess, 46.2ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.4ms preprocess, 44.7ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.6ms preprocess, 45.0ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 9.3ms preprocess, 45.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 16.2ms preprocess, 45.7ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 10.0ms preprocess, 47.3ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.1ms\n","Speed: 10.6ms preprocess, 46.1ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.3ms preprocess, 46.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.9ms\n","Speed: 8.6ms preprocess, 44.9ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 10.1ms preprocess, 46.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 8.9ms preprocess, 45.7ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 12.1ms preprocess, 45.8ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 9.6ms preprocess, 44.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 11.2ms preprocess, 45.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.6ms preprocess, 46.3ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.3ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 11.0ms preprocess, 44.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.0ms preprocess, 45.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.6ms preprocess, 45.6ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.2ms preprocess, 45.5ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.3ms preprocess, 46.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.7ms preprocess, 45.0ms inference, 6.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.0ms preprocess, 45.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.8ms preprocess, 44.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.9ms preprocess, 45.2ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 16.6ms preprocess, 43.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 10.0ms preprocess, 46.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.4ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.3ms preprocess, 44.5ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.7ms preprocess, 45.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.3ms preprocess, 46.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.1ms preprocess, 46.3ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.7ms preprocess, 44.9ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 11.9ms preprocess, 43.2ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.6ms preprocess, 45.9ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.4ms preprocess, 46.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.9ms preprocess, 45.3ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 10.2ms preprocess, 44.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 10.6ms preprocess, 45.5ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.4ms preprocess, 46.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.7ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.6ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.2ms preprocess, 46.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.0ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.5ms preprocess, 44.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.3ms preprocess, 46.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 10.8ms preprocess, 44.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 10.9ms preprocess, 46.6ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.0ms preprocess, 45.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.7ms preprocess, 45.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.1ms\n","Speed: 9.4ms preprocess, 45.1ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.7ms\n","Speed: 10.8ms preprocess, 46.7ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.8ms\n","Speed: 10.5ms preprocess, 46.8ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 11.2ms preprocess, 46.2ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.3ms\n","Speed: 10.3ms preprocess, 45.3ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 10.4ms preprocess, 45.5ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 48.3ms\n","Speed: 9.2ms preprocess, 48.3ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.6ms preprocess, 45.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.3ms preprocess, 46.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 10.1ms preprocess, 45.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 11.5ms preprocess, 44.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 10.2ms preprocess, 45.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.6ms\n","Speed: 10.8ms preprocess, 47.6ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 9.9ms preprocess, 45.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 9.7ms preprocess, 46.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.2ms preprocess, 45.8ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 12.2ms preprocess, 44.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.2ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.3ms preprocess, 45.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.0ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.2ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 10.2ms preprocess, 43.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 11.0ms preprocess, 45.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.7ms preprocess, 46.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 11.4ms preprocess, 44.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.3ms preprocess, 45.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 8.8ms preprocess, 44.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.5ms preprocess, 45.3ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.8ms preprocess, 46.6ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.5ms preprocess, 45.1ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 12.5ms preprocess, 44.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.4ms preprocess, 44.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 10.3ms preprocess, 46.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.0ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 11.3ms preprocess, 44.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 9.8ms preprocess, 44.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.6ms preprocess, 45.0ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.8ms preprocess, 46.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.0ms\n","Speed: 9.0ms preprocess, 47.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.7ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.6ms\n","Speed: 10.7ms preprocess, 44.6ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 10.4ms preprocess, 46.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.0ms preprocess, 45.6ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 8.9ms preprocess, 45.1ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.2ms\n","Speed: 9.2ms preprocess, 46.2ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.7ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.2ms\n","Speed: 10.4ms preprocess, 45.2ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.9ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.7ms preprocess, 45.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 8.7ms preprocess, 45.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.6ms preprocess, 46.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.9ms preprocess, 44.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.1ms preprocess, 45.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.1ms preprocess, 46.6ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.5ms preprocess, 45.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 8.7ms preprocess, 45.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.2ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 10.6ms preprocess, 43.2ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.2ms\n","Speed: 15.4ms preprocess, 44.2ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 10.8ms preprocess, 47.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 11.7ms preprocess, 45.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 10.6ms preprocess, 44.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 8.7ms preprocess, 46.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.5ms preprocess, 45.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.0ms preprocess, 44.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.5ms preprocess, 45.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.8ms preprocess, 45.2ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 11.3ms preprocess, 45.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.3ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.7ms preprocess, 44.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 10.4ms preprocess, 45.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.4ms preprocess, 46.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.1ms preprocess, 45.4ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.0ms preprocess, 44.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 9.5ms preprocess, 44.7ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 10.6ms preprocess, 46.8ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.4ms preprocess, 45.8ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 9.4ms preprocess, 45.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.2ms preprocess, 46.1ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 11.5ms preprocess, 45.1ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.6ms preprocess, 45.7ms inference, 6.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 12.5ms preprocess, 44.5ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.2ms preprocess, 44.7ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.4ms preprocess, 45.3ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 13.3ms preprocess, 44.3ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.7ms preprocess, 46.0ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.8ms preprocess, 45.0ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 11.2ms preprocess, 43.7ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.6ms preprocess, 46.3ms inference, 5.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 9.7ms preprocess, 45.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 9.5ms preprocess, 46.8ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.2ms\n","Speed: 10.3ms preprocess, 44.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.1ms\n","Speed: 10.4ms preprocess, 47.1ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.4ms preprocess, 44.5ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.2ms preprocess, 46.3ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.7ms preprocess, 45.2ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.9ms preprocess, 44.3ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.6ms preprocess, 45.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.5ms preprocess, 46.5ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.8ms preprocess, 46.2ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.3ms\n","Speed: 12.9ms preprocess, 43.3ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.1ms preprocess, 45.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.6ms preprocess, 46.0ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.0ms preprocess, 45.8ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.6ms preprocess, 45.7ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.6ms\n","Speed: 14.1ms preprocess, 43.6ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.8ms\n","Speed: 9.2ms preprocess, 47.8ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.3ms preprocess, 46.3ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.9ms preprocess, 46.7ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.3ms preprocess, 46.5ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.4ms preprocess, 45.1ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 11.3ms preprocess, 45.4ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.5ms\n","Speed: 12.6ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 10.3ms preprocess, 44.1ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.4ms preprocess, 46.0ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 13.8ms preprocess, 44.9ms inference, 5.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.4ms\n","Speed: 9.9ms preprocess, 43.4ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 13.1ms preprocess, 44.6ms inference, 5.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.9ms preprocess, 44.3ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.4ms\n","Speed: 12.3ms preprocess, 43.4ms inference, 7.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.7ms preprocess, 45.9ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.7ms preprocess, 45.7ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.4ms preprocess, 44.7ms inference, 5.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.1ms\n","Speed: 9.9ms preprocess, 47.1ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 15.0ms preprocess, 43.7ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 10.0ms preprocess, 43.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 12.1ms preprocess, 44.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.0ms preprocess, 46.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 9.7ms preprocess, 44.2ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.3ms preprocess, 46.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.8ms preprocess, 45.9ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.6ms\n","Speed: 12.0ms preprocess, 43.6ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 12.7ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.6ms preprocess, 46.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.7ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.0ms\n","Speed: 10.2ms preprocess, 45.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.4ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.7ms preprocess, 46.5ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 9.9ms preprocess, 45.7ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.5ms\n","Speed: 9.4ms preprocess, 46.5ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.6ms preprocess, 46.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.9ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.4ms preprocess, 45.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.2ms preprocess, 44.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.8ms preprocess, 46.1ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 11.5ms preprocess, 45.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.7ms preprocess, 46.0ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.5ms\n","Speed: 13.7ms preprocess, 42.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.3ms preprocess, 46.0ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.8ms\n","Speed: 9.2ms preprocess, 46.8ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.5ms\n","Speed: 10.6ms preprocess, 46.5ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.3ms preprocess, 46.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 8.0ms preprocess, 44.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.2ms preprocess, 45.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 11.1ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.2ms preprocess, 45.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.2ms preprocess, 44.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 9.8ms preprocess, 44.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 10.0ms preprocess, 46.4ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.1ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.6ms preprocess, 45.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.5ms preprocess, 44.7ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 11.7ms preprocess, 44.5ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.0ms\n","Speed: 12.7ms preprocess, 44.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.4ms preprocess, 46.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.2ms preprocess, 46.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 10.3ms preprocess, 43.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.5ms\n","Speed: 9.1ms preprocess, 47.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.3ms preprocess, 45.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.6ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.9ms\n","Speed: 10.7ms preprocess, 44.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.2ms preprocess, 45.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 8.8ms preprocess, 44.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.6ms preprocess, 45.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.3ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 10.1ms preprocess, 44.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 12.7ms preprocess, 44.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 11.4ms preprocess, 45.1ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 9.2ms preprocess, 44.0ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 8.9ms preprocess, 44.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.9ms preprocess, 45.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.6ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.8ms preprocess, 45.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.1ms preprocess, 46.0ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 42.2ms\n","Speed: 9.8ms preprocess, 42.2ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 48.4ms\n","Speed: 9.5ms preprocess, 48.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 8.5ms preprocess, 46.2ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 8.9ms preprocess, 45.4ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 7.3ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.1ms preprocess, 46.5ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.4ms preprocess, 45.1ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.5ms preprocess, 45.2ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.3ms preprocess, 45.2ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 11.2ms preprocess, 45.9ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.4ms preprocess, 45.9ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 8.7ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.0ms preprocess, 46.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.5ms preprocess, 45.3ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 8.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.4ms preprocess, 45.6ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.7ms preprocess, 45.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.2ms\n","Speed: 9.0ms preprocess, 46.2ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.2ms preprocess, 46.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.0ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.7ms preprocess, 45.1ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.1ms preprocess, 46.1ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.2ms preprocess, 45.3ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.3ms preprocess, 44.8ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.6ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 8.4ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.0ms preprocess, 46.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.1ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.4ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 8.7ms preprocess, 44.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.2ms\n","Speed: 9.0ms preprocess, 46.2ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 9.9ms preprocess, 47.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.4ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 8.8ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.4ms preprocess, 45.6ms inference, 2.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.4ms preprocess, 46.0ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.7ms preprocess, 46.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 9.7ms preprocess, 44.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 9.0ms preprocess, 45.8ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 9.5ms preprocess, 46.4ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.7ms preprocess, 45.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.9ms preprocess, 46.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.3ms preprocess, 46.7ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.4ms\n","Speed: 11.1ms preprocess, 43.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.2ms preprocess, 46.2ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.9ms preprocess, 45.6ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.3ms\n","Speed: 9.0ms preprocess, 46.3ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.0ms preprocess, 46.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.3ms\n","Speed: 9.3ms preprocess, 46.3ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.6ms preprocess, 46.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 9.4ms preprocess, 45.6ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.1ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.3ms\n","Speed: 14.4ms preprocess, 43.3ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 10.2ms preprocess, 46.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.8ms preprocess, 44.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.3ms preprocess, 45.1ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 11.8ms preprocess, 45.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.4ms preprocess, 46.0ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.2ms preprocess, 45.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.0ms preprocess, 45.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.4ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.6ms preprocess, 46.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.5ms preprocess, 45.9ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 8.8ms preprocess, 46.5ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.0ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.1ms\n","Speed: 9.3ms preprocess, 46.1ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 8.6ms preprocess, 46.8ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 13.2ms preprocess, 44.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 10.4ms preprocess, 45.9ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 11.4ms preprocess, 45.8ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 11.0ms preprocess, 44.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.0ms preprocess, 45.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 8.8ms preprocess, 46.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 8.7ms preprocess, 45.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 9.2ms preprocess, 45.5ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.7ms preprocess, 45.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 8.9ms preprocess, 45.4ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 8.9ms preprocess, 45.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.4ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.4ms preprocess, 46.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.5ms preprocess, 46.5ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.9ms preprocess, 45.2ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 12.1ms preprocess, 44.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.7ms preprocess, 45.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 8.8ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 10.1ms preprocess, 45.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.9ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.1ms preprocess, 45.6ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 9.4ms preprocess, 45.8ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.3ms preprocess, 46.2ms inference, 5.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 8.6ms preprocess, 44.3ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 11.3ms preprocess, 45.6ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.1ms preprocess, 46.0ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.0ms preprocess, 45.2ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 9.7ms preprocess, 43.8ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 14.2ms preprocess, 45.6ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 12.7ms preprocess, 44.8ms inference, 7.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.5ms\n","Speed: 12.4ms preprocess, 43.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 9.2ms preprocess, 45.7ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.6ms preprocess, 45.9ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.4ms preprocess, 45.3ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.2ms preprocess, 44.7ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.2ms\n","Speed: 9.0ms preprocess, 46.2ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.0ms preprocess, 46.4ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.2ms preprocess, 45.8ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.5ms preprocess, 45.3ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.6ms preprocess, 44.7ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.7ms preprocess, 45.0ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.1ms\n","Speed: 11.0ms preprocess, 45.1ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.8ms preprocess, 45.7ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.6ms preprocess, 45.2ms inference, 6.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 9.9ms preprocess, 44.7ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.9ms preprocess, 45.8ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 11.5ms preprocess, 44.9ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.7ms preprocess, 46.0ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.4ms preprocess, 46.1ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.2ms preprocess, 45.7ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 11.3ms preprocess, 43.2ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 9.8ms preprocess, 44.1ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.6ms preprocess, 46.0ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.2ms preprocess, 45.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.5ms preprocess, 46.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 11.7ms preprocess, 44.0ms inference, 8.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.4ms preprocess, 44.3ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 11.1ms preprocess, 44.9ms inference, 6.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 9.1ms preprocess, 44.1ms inference, 5.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.3ms preprocess, 45.6ms inference, 6.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.9ms preprocess, 44.9ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.7ms\n","Speed: 14.6ms preprocess, 43.7ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.7ms\n","Speed: 9.8ms preprocess, 47.7ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.2ms\n","Speed: 11.2ms preprocess, 45.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 13.5ms preprocess, 43.2ms inference, 5.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.9ms preprocess, 46.4ms inference, 9.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 41.6ms\n","Speed: 11.0ms preprocess, 41.6ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 8.4ms preprocess, 45.6ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.1ms preprocess, 45.7ms inference, 5.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.1ms preprocess, 46.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.9ms preprocess, 46.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.8ms preprocess, 46.0ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.5ms preprocess, 44.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.8ms preprocess, 46.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.3ms preprocess, 45.0ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.2ms preprocess, 44.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 10.6ms preprocess, 46.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.6ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 8.6ms preprocess, 46.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.7ms preprocess, 44.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.8ms\n","Speed: 9.5ms preprocess, 43.8ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 10.2ms preprocess, 47.3ms inference, 6.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.6ms\n","Speed: 11.6ms preprocess, 44.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 9.3ms preprocess, 46.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.0ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.3ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.9ms\n","Speed: 9.2ms preprocess, 42.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.1ms preprocess, 46.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 9.4ms preprocess, 44.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.0ms preprocess, 46.3ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.9ms\n","Speed: 14.5ms preprocess, 42.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 11.4ms preprocess, 44.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.9ms preprocess, 45.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 9.1ms preprocess, 46.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 9.5ms preprocess, 44.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.9ms\n","Speed: 10.7ms preprocess, 44.9ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 8.9ms preprocess, 46.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.3ms preprocess, 46.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 11.2ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.7ms preprocess, 45.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.5ms preprocess, 45.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.6ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 10.9ms preprocess, 45.9ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 8.4ms preprocess, 46.6ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 7.6ms preprocess, 46.4ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.9ms\n","Speed: 11.6ms preprocess, 42.9ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 7.9ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 10.4ms preprocess, 45.8ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 7.7ms preprocess, 45.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.7ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 8.9ms preprocess, 46.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 8.6ms preprocess, 46.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.8ms preprocess, 45.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.6ms preprocess, 45.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.1ms preprocess, 46.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 10.5ms preprocess, 44.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.7ms preprocess, 45.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 8.2ms preprocess, 43.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.9ms preprocess, 46.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.5ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.7ms preprocess, 45.5ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.0ms\n","Speed: 11.6ms preprocess, 43.0ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 11.3ms preprocess, 45.7ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.2ms preprocess, 46.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 8.3ms preprocess, 46.0ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 10.2ms preprocess, 46.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 9.6ms preprocess, 45.7ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.6ms preprocess, 46.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.0ms preprocess, 44.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.3ms preprocess, 45.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.2ms preprocess, 45.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 8.6ms preprocess, 46.6ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 8.4ms preprocess, 46.0ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.6ms preprocess, 45.2ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.6ms preprocess, 45.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 9.9ms preprocess, 44.6ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.5ms preprocess, 45.5ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 8.2ms preprocess, 46.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.0ms preprocess, 46.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.7ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.4ms preprocess, 45.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.6ms preprocess, 44.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.0ms preprocess, 45.8ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.5ms preprocess, 44.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.1ms preprocess, 45.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 8.8ms preprocess, 47.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.3ms preprocess, 44.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.5ms preprocess, 45.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.5ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.1ms preprocess, 46.0ms inference, 2.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.0ms preprocess, 46.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.4ms\n","Speed: 8.4ms preprocess, 43.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 11.2ms preprocess, 46.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.8ms preprocess, 45.9ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 10.1ms preprocess, 44.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 9.8ms preprocess, 45.4ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.4ms preprocess, 45.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 8.0ms preprocess, 47.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.1ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 8.9ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.5ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 8.9ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.1ms preprocess, 46.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 8.2ms preprocess, 46.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.5ms preprocess, 44.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.8ms preprocess, 46.6ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.0ms\n","Speed: 9.5ms preprocess, 45.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.1ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 8.8ms preprocess, 46.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 9.3ms preprocess, 44.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.9ms preprocess, 46.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 10.0ms preprocess, 44.7ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 8.2ms preprocess, 46.6ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.9ms preprocess, 45.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.7ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.6ms preprocess, 44.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 7.8ms preprocess, 44.7ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.7ms\n","Speed: 9.1ms preprocess, 47.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.9ms preprocess, 46.3ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.7ms preprocess, 46.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.1ms preprocess, 45.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 10.2ms preprocess, 44.5ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 10.1ms preprocess, 44.4ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.3ms preprocess, 45.5ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 10.0ms preprocess, 44.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 8.9ms preprocess, 46.5ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.2ms preprocess, 45.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.2ms preprocess, 46.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.1ms\n","Speed: 10.2ms preprocess, 46.1ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.5ms preprocess, 46.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 9.7ms preprocess, 45.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.2ms preprocess, 46.3ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 8.7ms preprocess, 45.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 11.4ms preprocess, 45.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.5ms preprocess, 45.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.9ms preprocess, 45.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.1ms preprocess, 44.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 10.2ms preprocess, 43.8ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 13.1ms preprocess, 45.8ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 8.5ms preprocess, 45.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.4ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.7ms preprocess, 45.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.2ms preprocess, 44.8ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 8.7ms preprocess, 45.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.1ms\n","Speed: 8.5ms preprocess, 47.1ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 8.2ms preprocess, 45.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.3ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.2ms preprocess, 45.9ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 8.8ms preprocess, 46.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.1ms\n","Speed: 8.7ms preprocess, 47.1ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.1ms preprocess, 44.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 9.0ms preprocess, 45.4ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.9ms preprocess, 46.1ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.9ms preprocess, 46.1ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.2ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 9.2ms preprocess, 45.6ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.4ms\n","Speed: 9.0ms preprocess, 47.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.1ms\n","Speed: 9.1ms preprocess, 44.1ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 10.5ms preprocess, 46.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.8ms preprocess, 45.9ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.5ms preprocess, 45.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 11.0ms preprocess, 44.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.1ms\n","Speed: 9.5ms preprocess, 47.1ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.0ms preprocess, 46.7ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.4ms\n","Speed: 12.3ms preprocess, 43.4ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.1ms\n","Speed: 14.8ms preprocess, 43.1ms inference, 7.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.2ms\n","Speed: 9.1ms preprocess, 47.2ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.7ms preprocess, 45.5ms inference, 6.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.9ms\n","Speed: 12.5ms preprocess, 42.9ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.6ms\n","Speed: 10.7ms preprocess, 44.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.0ms\n","Speed: 14.4ms preprocess, 43.0ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 13.0ms preprocess, 44.8ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.7ms preprocess, 45.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.9ms preprocess, 44.3ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 10.7ms preprocess, 44.5ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.2ms\n","Speed: 9.0ms preprocess, 47.2ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.9ms preprocess, 45.6ms inference, 5.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.5ms preprocess, 44.3ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.5ms preprocess, 46.4ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.1ms preprocess, 46.1ms inference, 6.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.5ms\n","Speed: 13.9ms preprocess, 43.5ms inference, 7.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 12.3ms preprocess, 44.0ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.0ms\n","Speed: 12.8ms preprocess, 43.0ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.2ms preprocess, 44.7ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 11.2ms preprocess, 46.0ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 10.2ms preprocess, 44.2ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.9ms preprocess, 44.9ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 15.5ms preprocess, 45.0ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.0ms preprocess, 45.1ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 10.4ms preprocess, 44.2ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.0ms preprocess, 45.7ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 12.0ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.9ms\n","Speed: 11.9ms preprocess, 44.9ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 12.7ms preprocess, 44.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 11.0ms preprocess, 46.5ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.2ms preprocess, 45.8ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.5ms\n","Speed: 9.0ms preprocess, 44.5ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 10.8ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.9ms preprocess, 45.7ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 12.5ms preprocess, 45.7ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.4ms\n","Speed: 11.4ms preprocess, 47.4ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.9ms\n","Speed: 12.4ms preprocess, 43.9ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 12.5ms preprocess, 46.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.6ms preprocess, 46.2ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 9.7ms preprocess, 44.3ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.0ms\n","Speed: 13.4ms preprocess, 43.0ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.2ms\n","Speed: 12.9ms preprocess, 43.2ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.5ms preprocess, 44.5ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 9.6ms preprocess, 43.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.0ms\n","Speed: 13.4ms preprocess, 43.0ms inference, 5.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.8ms\n","Speed: 9.9ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 16.5ms preprocess, 45.0ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.7ms preprocess, 44.8ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 9.7ms preprocess, 43.8ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 11.1ms preprocess, 46.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 10.1ms preprocess, 44.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 10.9ms preprocess, 44.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 10.4ms preprocess, 44.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.8ms preprocess, 45.5ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 10.0ms preprocess, 44.8ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.3ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 9.4ms preprocess, 46.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 8.9ms preprocess, 44.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.7ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.3ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.1ms preprocess, 45.4ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 13.0ms preprocess, 44.8ms inference, 11.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.8ms\n","Speed: 12.3ms preprocess, 42.8ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 8.8ms preprocess, 47.3ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 9.0ms preprocess, 44.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 11.1ms preprocess, 46.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.1ms preprocess, 46.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 10.4ms preprocess, 44.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.7ms preprocess, 44.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.4ms preprocess, 46.2ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 10.3ms preprocess, 43.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.6ms preprocess, 46.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.5ms preprocess, 45.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 8.7ms preprocess, 45.2ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.4ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.0ms preprocess, 46.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 11.0ms preprocess, 45.2ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.8ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.2ms\n","Speed: 9.9ms preprocess, 45.2ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.1ms preprocess, 44.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 10.3ms preprocess, 46.0ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.5ms preprocess, 46.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.1ms\n","Speed: 8.5ms preprocess, 45.1ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.8ms preprocess, 46.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.6ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.6ms preprocess, 45.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.2ms\n","Speed: 8.4ms preprocess, 47.2ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.4ms\n","Speed: 8.7ms preprocess, 47.4ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 11.2ms preprocess, 44.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.7ms\n","Speed: 9.1ms preprocess, 46.7ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 8.7ms preprocess, 46.4ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.1ms preprocess, 46.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 11.5ms preprocess, 47.3ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.1ms preprocess, 44.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.2ms preprocess, 45.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.8ms preprocess, 46.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.8ms preprocess, 46.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.3ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.3ms preprocess, 44.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.6ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.4ms preprocess, 45.4ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 8.9ms preprocess, 46.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.1ms\n","Speed: 8.7ms preprocess, 46.1ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 8.6ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.6ms preprocess, 46.3ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.7ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 11.2ms preprocess, 46.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.5ms preprocess, 45.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.9ms preprocess, 44.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.9ms preprocess, 45.7ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 8.3ms preprocess, 44.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.1ms preprocess, 46.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.7ms preprocess, 45.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.4ms preprocess, 45.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 48.9ms\n","Speed: 8.2ms preprocess, 48.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.3ms preprocess, 45.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.2ms\n","Speed: 9.0ms preprocess, 46.2ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.6ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.6ms preprocess, 46.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 9.2ms preprocess, 45.7ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 8.9ms preprocess, 46.0ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.9ms\n","Speed: 11.6ms preprocess, 43.9ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 49.0ms\n","Speed: 8.9ms preprocess, 49.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.1ms preprocess, 45.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.3ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.0ms preprocess, 45.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.4ms preprocess, 45.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 10.7ms preprocess, 44.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.5ms preprocess, 45.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 8.7ms preprocess, 45.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.0ms preprocess, 45.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 8.6ms preprocess, 45.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 9.1ms preprocess, 44.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.7ms\n","Speed: 9.2ms preprocess, 47.7ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.6ms\n","Speed: 10.4ms preprocess, 43.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.5ms\n","Speed: 10.5ms preprocess, 44.5ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 10.6ms preprocess, 46.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 8.9ms preprocess, 45.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.8ms preprocess, 45.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.3ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 11.5ms preprocess, 44.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.6ms\n","Speed: 9.7ms preprocess, 44.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.6ms\n","Speed: 10.2ms preprocess, 47.6ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 10.2ms preprocess, 46.9ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.6ms preprocess, 45.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.4ms preprocess, 45.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 10.1ms preprocess, 45.5ms inference, 2.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.0ms preprocess, 45.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.7ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 12.0ms preprocess, 46.2ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.4ms\n","Speed: 12.5ms preprocess, 43.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.3ms preprocess, 45.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.4ms preprocess, 45.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.0ms preprocess, 45.9ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.8ms\n","Speed: 8.7ms preprocess, 47.8ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 9.6ms preprocess, 46.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.0ms preprocess, 46.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 12.2ms preprocess, 44.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 42.9ms\n","Speed: 11.2ms preprocess, 42.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.4ms\n","Speed: 9.5ms preprocess, 47.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.8ms preprocess, 44.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 9.5ms preprocess, 46.4ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.2ms preprocess, 45.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.6ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 8.9ms preprocess, 45.0ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.8ms preprocess, 46.1ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 10.5ms preprocess, 44.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 10.8ms preprocess, 44.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.5ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.4ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.7ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.4ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 8.8ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 8.5ms preprocess, 45.4ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.3ms preprocess, 46.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 9.8ms preprocess, 45.5ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.4ms preprocess, 45.9ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 11.1ms preprocess, 45.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.2ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.5ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 13.2ms preprocess, 44.6ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 12.0ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.4ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.5ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 13.1ms preprocess, 44.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.8ms preprocess, 45.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 10.3ms preprocess, 46.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 9.3ms preprocess, 46.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 9.2ms preprocess, 43.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 8.9ms preprocess, 45.0ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 11.2ms preprocess, 45.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.2ms preprocess, 46.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.3ms\n","Speed: 9.2ms preprocess, 46.3ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.3ms\n","Speed: 8.6ms preprocess, 45.3ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.8ms preprocess, 46.2ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.0ms\n","Speed: 10.9ms preprocess, 43.0ms inference, 5.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 12.5ms preprocess, 45.0ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 13.2ms preprocess, 46.1ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 10.8ms preprocess, 43.8ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 11.4ms preprocess, 45.1ms inference, 6.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.1ms\n","Speed: 9.2ms preprocess, 47.1ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 10.6ms preprocess, 45.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 9.3ms preprocess, 44.0ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 11.9ms preprocess, 45.1ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 42.5ms\n","Speed: 11.3ms preprocess, 42.5ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.1ms preprocess, 46.3ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 9.3ms preprocess, 44.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.7ms preprocess, 46.0ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.7ms preprocess, 45.1ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.6ms preprocess, 44.9ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.8ms\n","Speed: 10.4ms preprocess, 44.8ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.5ms\n","Speed: 10.3ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 11.2ms preprocess, 45.0ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.9ms preprocess, 46.1ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.0ms\n","Speed: 9.8ms preprocess, 45.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 16.1ms preprocess, 43.9ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.8ms preprocess, 45.4ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.6ms preprocess, 44.3ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.1ms\n","Speed: 14.2ms preprocess, 43.1ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 11.1ms preprocess, 45.0ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 12.5ms preprocess, 45.6ms inference, 6.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.5ms\n","Speed: 12.0ms preprocess, 43.5ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 10.0ms preprocess, 44.5ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.9ms preprocess, 46.1ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.8ms preprocess, 45.9ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.5ms preprocess, 45.4ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 8.8ms preprocess, 45.4ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.7ms preprocess, 46.0ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 41.7ms\n","Speed: 16.6ms preprocess, 41.7ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 13.8ms preprocess, 44.6ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.7ms preprocess, 46.1ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.6ms preprocess, 45.0ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.5ms preprocess, 45.2ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.8ms\n","Speed: 14.2ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 11.8ms preprocess, 46.3ms inference, 5.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.4ms\n","Speed: 9.8ms preprocess, 43.4ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 10.8ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 14.6ms preprocess, 44.9ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 11.6ms preprocess, 44.0ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.9ms\n","Speed: 12.5ms preprocess, 43.9ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 12.0ms preprocess, 44.7ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.9ms preprocess, 45.1ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 11.5ms preprocess, 44.9ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.7ms\n","Speed: 11.8ms preprocess, 42.7ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.4ms preprocess, 45.7ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 12.9ms preprocess, 44.4ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.2ms\n","Speed: 12.8ms preprocess, 44.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.7ms preprocess, 46.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.3ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.3ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.4ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.2ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.4ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.6ms\n","Speed: 9.5ms preprocess, 45.6ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 9.7ms preprocess, 47.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.7ms preprocess, 45.7ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.8ms preprocess, 44.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.3ms preprocess, 45.3ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.6ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 11.5ms preprocess, 45.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.9ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.0ms preprocess, 45.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.4ms preprocess, 45.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.6ms preprocess, 45.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.8ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 11.0ms preprocess, 46.0ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 8.9ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 10.3ms preprocess, 44.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.9ms preprocess, 46.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.5ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 11.0ms preprocess, 46.1ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.2ms\n","Speed: 10.6ms preprocess, 44.2ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.4ms preprocess, 46.2ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.1ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 8.3ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 8.9ms preprocess, 45.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.7ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.8ms preprocess, 46.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.1ms preprocess, 45.9ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.3ms preprocess, 46.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.5ms preprocess, 45.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.5ms preprocess, 45.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 12.1ms preprocess, 44.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.8ms preprocess, 46.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 11.1ms preprocess, 45.9ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.5ms preprocess, 46.3ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 10.4ms preprocess, 44.1ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.8ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 11.0ms preprocess, 44.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.6ms\n","Speed: 11.5ms preprocess, 43.6ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 10.4ms preprocess, 45.5ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.2ms\n","Speed: 8.9ms preprocess, 47.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.2ms preprocess, 44.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 10.3ms preprocess, 45.8ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 9.2ms preprocess, 47.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 9.2ms preprocess, 45.4ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 48.3ms\n","Speed: 9.3ms preprocess, 48.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 11.4ms preprocess, 44.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 11.1ms preprocess, 45.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 10.4ms preprocess, 44.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.5ms preprocess, 45.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 11.0ms preprocess, 44.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.9ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.6ms preprocess, 46.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.8ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 8.8ms preprocess, 47.3ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.6ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.6ms preprocess, 45.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.1ms preprocess, 45.9ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.1ms\n","Speed: 10.3ms preprocess, 44.1ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.5ms\n","Speed: 15.8ms preprocess, 43.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 9.6ms preprocess, 46.4ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.2ms preprocess, 46.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.4ms preprocess, 44.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.4ms preprocess, 44.9ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.0ms\n","Speed: 9.4ms preprocess, 47.0ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 8.6ms preprocess, 45.8ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.0ms preprocess, 45.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.3ms\n","Speed: 10.0ms preprocess, 45.3ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.8ms preprocess, 46.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.8ms preprocess, 44.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.2ms preprocess, 46.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.3ms preprocess, 46.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.2ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.7ms preprocess, 45.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.6ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.8ms preprocess, 45.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.6ms preprocess, 46.4ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.1ms preprocess, 46.3ms inference, 9.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 9.0ms preprocess, 44.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.0ms\n","Speed: 13.1ms preprocess, 44.0ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 8.6ms preprocess, 45.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 10.0ms preprocess, 45.7ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.3ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.7ms preprocess, 45.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.2ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.5ms preprocess, 45.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 8.7ms preprocess, 46.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.1ms preprocess, 46.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.6ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 9.3ms preprocess, 45.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 12.6ms preprocess, 44.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.0ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.6ms preprocess, 45.1ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 11.0ms preprocess, 44.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.8ms\n","Speed: 9.2ms preprocess, 43.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.8ms\n","Speed: 9.5ms preprocess, 44.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 12.1ms preprocess, 45.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 9.4ms preprocess, 45.5ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.5ms\n","Speed: 9.6ms preprocess, 46.5ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 8.9ms preprocess, 46.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.0ms preprocess, 46.5ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.6ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.2ms preprocess, 46.5ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 12.4ms preprocess, 43.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 10.1ms preprocess, 45.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.7ms preprocess, 46.4ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.2ms preprocess, 45.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 8.9ms preprocess, 46.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.8ms\n","Speed: 9.2ms preprocess, 45.8ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.1ms preprocess, 45.6ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.3ms\n","Speed: 11.8ms preprocess, 44.3ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 13.1ms preprocess, 46.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.7ms preprocess, 45.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.8ms preprocess, 45.7ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.2ms preprocess, 45.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 8.8ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 8.9ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 8.9ms preprocess, 46.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.2ms preprocess, 46.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 8.7ms preprocess, 46.0ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.1ms preprocess, 45.2ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 8.6ms preprocess, 46.5ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.4ms preprocess, 45.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 8.0ms preprocess, 46.2ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.6ms preprocess, 45.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.5ms\n","Speed: 9.0ms preprocess, 46.5ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 11.6ms preprocess, 46.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.9ms preprocess, 45.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.8ms\n","Speed: 10.6ms preprocess, 44.8ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.1ms preprocess, 46.3ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 10.1ms preprocess, 46.0ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.4ms preprocess, 46.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 9.5ms preprocess, 45.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.1ms preprocess, 44.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.5ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.1ms\n","Speed: 9.5ms preprocess, 47.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.4ms\n","Speed: 9.6ms preprocess, 47.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.7ms preprocess, 45.3ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 10.3ms preprocess, 43.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.7ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.7ms preprocess, 44.8ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 11.8ms preprocess, 45.2ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.1ms preprocess, 45.0ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.6ms preprocess, 45.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.3ms preprocess, 45.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 8.2ms preprocess, 45.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.1ms\n","Speed: 9.1ms preprocess, 46.1ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.5ms preprocess, 46.2ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.3ms preprocess, 45.0ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.8ms preprocess, 44.4ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 11.4ms preprocess, 46.1ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.6ms preprocess, 44.5ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.2ms\n","Speed: 9.3ms preprocess, 47.2ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.3ms preprocess, 44.9ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 12.1ms preprocess, 45.0ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.0ms\n","Speed: 13.0ms preprocess, 44.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 11.3ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 11.5ms preprocess, 45.6ms inference, 6.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.6ms\n","Speed: 9.2ms preprocess, 47.6ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 12.3ms preprocess, 45.0ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 50.1ms\n","Speed: 11.4ms preprocess, 50.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.6ms\n","Speed: 10.1ms preprocess, 43.6ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.8ms preprocess, 46.1ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.0ms preprocess, 46.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 10.0ms preprocess, 46.2ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 9.4ms preprocess, 44.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 9.5ms preprocess, 43.8ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.7ms preprocess, 45.8ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 11.0ms preprocess, 46.0ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 10.9ms preprocess, 44.0ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 9.7ms preprocess, 46.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.1ms preprocess, 46.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.6ms\n","Speed: 13.1ms preprocess, 43.6ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.2ms preprocess, 46.2ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.1ms preprocess, 45.2ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.7ms preprocess, 46.1ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 13.8ms preprocess, 45.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.5ms preprocess, 45.1ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.1ms preprocess, 44.7ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 11.4ms preprocess, 45.6ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.4ms preprocess, 45.9ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.0ms preprocess, 45.4ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 8.5ms preprocess, 46.5ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.5ms preprocess, 46.2ms inference, 4.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 9.0ms preprocess, 44.8ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 11.6ms preprocess, 44.6ms inference, 8.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.6ms\n","Speed: 11.1ms preprocess, 43.6ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 11.5ms preprocess, 44.8ms inference, 6.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.6ms\n","Speed: 9.2ms preprocess, 43.6ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 13.2ms preprocess, 44.5ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.3ms\n","Speed: 13.7ms preprocess, 44.3ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.9ms\n","Speed: 11.8ms preprocess, 44.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 9.9ms preprocess, 45.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.8ms preprocess, 46.2ms inference, 4.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.5ms preprocess, 44.9ms inference, 5.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.9ms\n","Speed: 9.8ms preprocess, 42.9ms inference, 5.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.5ms preprocess, 45.2ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.0ms preprocess, 46.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.9ms preprocess, 46.1ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.4ms preprocess, 44.7ms inference, 316.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 39.6ms\n","Speed: 9.9ms preprocess, 39.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.1ms preprocess, 46.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.2ms preprocess, 45.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.2ms preprocess, 45.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.4ms preprocess, 45.0ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 10.7ms preprocess, 45.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.0ms\n","Speed: 9.8ms preprocess, 46.0ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.3ms preprocess, 45.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.1ms preprocess, 46.1ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 8.9ms preprocess, 45.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.3ms preprocess, 46.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.8ms preprocess, 46.1ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.5ms\n","Speed: 11.8ms preprocess, 45.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.6ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.4ms preprocess, 45.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.4ms preprocess, 45.6ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.3ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.2ms preprocess, 46.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 11.1ms preprocess, 45.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.6ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.5ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.5ms preprocess, 46.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.9ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.0ms preprocess, 45.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 9.6ms preprocess, 45.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 8.9ms preprocess, 45.6ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.0ms preprocess, 45.8ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.3ms\n","Speed: 13.2ms preprocess, 43.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.4ms preprocess, 46.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.1ms preprocess, 45.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 8.9ms preprocess, 44.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.5ms preprocess, 45.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 11.5ms preprocess, 44.9ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 8.6ms preprocess, 46.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.8ms preprocess, 46.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.9ms\n","Speed: 9.2ms preprocess, 46.9ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.1ms\n","Speed: 9.4ms preprocess, 47.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.5ms preprocess, 46.5ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 8.7ms preprocess, 46.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 8.7ms preprocess, 46.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 10.1ms preprocess, 45.4ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 47.0ms\n","Speed: 9.3ms preprocess, 47.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.1ms preprocess, 46.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.1ms\n","Speed: 12.7ms preprocess, 45.1ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 10.3ms preprocess, 46.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.1ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.3ms preprocess, 45.9ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.1ms\n","Speed: 9.2ms preprocess, 45.1ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.3ms preprocess, 46.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.2ms preprocess, 45.8ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 11.6ms preprocess, 46.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 9.7ms preprocess, 44.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 9.6ms preprocess, 45.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 8.9ms preprocess, 45.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.2ms preprocess, 46.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.7ms preprocess, 46.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 9.0ms preprocess, 45.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.2ms preprocess, 45.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.8ms\n","Speed: 10.6ms preprocess, 43.8ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.6ms preprocess, 45.1ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.4ms preprocess, 46.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.2ms preprocess, 46.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.7ms preprocess, 46.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.3ms\n","Speed: 10.0ms preprocess, 45.3ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.4ms preprocess, 46.4ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.1ms preprocess, 44.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.6ms\n","Speed: 10.2ms preprocess, 45.6ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.3ms preprocess, 46.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.5ms preprocess, 45.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 8.4ms preprocess, 46.2ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.1ms preprocess, 45.8ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.0ms preprocess, 45.9ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 9.3ms preprocess, 46.4ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 8.7ms preprocess, 46.2ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 10.2ms preprocess, 45.3ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.5ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.0ms\n","Speed: 9.7ms preprocess, 44.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.1ms preprocess, 45.9ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 10.1ms preprocess, 46.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 10.0ms preprocess, 46.5ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.3ms preprocess, 45.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.5ms preprocess, 46.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 8.9ms preprocess, 45.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 10.0ms preprocess, 44.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 10.2ms preprocess, 45.9ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.0ms preprocess, 46.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 10.6ms preprocess, 45.7ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 10.1ms preprocess, 46.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.9ms preprocess, 45.0ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 9.5ms preprocess, 46.3ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 11.3ms preprocess, 45.4ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.8ms\n","Speed: 9.5ms preprocess, 46.8ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.4ms preprocess, 46.0ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.5ms preprocess, 45.3ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.2ms\n","Speed: 10.1ms preprocess, 45.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 9.3ms preprocess, 44.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.8ms preprocess, 45.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 9.5ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.4ms preprocess, 46.4ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.8ms preprocess, 45.9ms inference, 3.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.7ms\n","Speed: 8.6ms preprocess, 44.7ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.9ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.2ms\n","Speed: 9.7ms preprocess, 45.2ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 11.2ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.2ms preprocess, 45.9ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.9ms\n","Speed: 10.6ms preprocess, 44.9ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 44.3ms\n","Speed: 10.0ms preprocess, 44.3ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.3ms\n","Speed: 9.1ms preprocess, 47.3ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 11.0ms preprocess, 44.5ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.3ms\n","Speed: 11.7ms preprocess, 44.3ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 43.7ms\n","Speed: 9.7ms preprocess, 43.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 9.5ms preprocess, 46.6ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.5ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 8.9ms preprocess, 46.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 8.7ms preprocess, 46.4ms inference, 2.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.7ms\n","Speed: 9.1ms preprocess, 45.7ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.8ms\n","Speed: 8.8ms preprocess, 46.8ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.3ms\n","Speed: 8.7ms preprocess, 46.3ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.5ms\n","Speed: 9.6ms preprocess, 46.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 9.1ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.5ms\n","Speed: 9.3ms preprocess, 45.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.7ms preprocess, 45.7ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.1ms\n","Speed: 13.7ms preprocess, 42.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.5ms\n","Speed: 9.3ms preprocess, 46.5ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.9ms preprocess, 45.7ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.7ms\n","Speed: 10.2ms preprocess, 45.7ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.2ms\n","Speed: 9.0ms preprocess, 46.2ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.0ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.3ms\n","Speed: 9.0ms preprocess, 45.3ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.4ms\n","Speed: 8.9ms preprocess, 45.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 9.9ms preprocess, 45.8ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 9.5ms preprocess, 44.5ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 46.4ms\n","Speed: 8.9ms preprocess, 46.4ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.0ms preprocess, 46.1ms inference, 2.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.7ms\n","Speed: 10.2ms preprocess, 44.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.4ms\n","Speed: 10.5ms preprocess, 45.4ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.0ms\n","Speed: 9.7ms preprocess, 46.0ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.8ms\n","Speed: 11.1ms preprocess, 44.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 11.1ms preprocess, 45.8ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.3ms\n","Speed: 10.2ms preprocess, 45.3ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 48.4ms\n","Speed: 9.7ms preprocess, 48.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 45.9ms\n","Speed: 10.0ms preprocess, 45.9ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.4ms\n","Speed: 10.6ms preprocess, 46.4ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 10.2ms preprocess, 44.6ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.9ms\n","Speed: 8.8ms preprocess, 45.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.7ms\n","Speed: 9.2ms preprocess, 46.7ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.6ms preprocess, 46.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.8ms\n","Speed: 10.2ms preprocess, 45.8ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.3ms\n","Speed: 9.7ms preprocess, 43.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.9ms\n","Speed: 9.2ms preprocess, 46.9ms inference, 4.3ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 42.6ms\n","Speed: 11.9ms preprocess, 42.6ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 (no detections), 43.7ms\n","Speed: 11.9ms preprocess, 43.7ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 47.4ms\n","Speed: 9.7ms preprocess, 47.4ms inference, 4.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.6ms\n","Speed: 8.6ms preprocess, 44.6ms inference, 4.2ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 10.4ms preprocess, 45.0ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.4ms\n","Speed: 9.2ms preprocess, 44.4ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.5ms\n","Speed: 11.1ms preprocess, 44.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.6ms\n","Speed: 8.9ms preprocess, 46.6ms inference, 3.4ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 46.1ms\n","Speed: 9.1ms preprocess, 46.1ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.1ms\n","Speed: 9.2ms preprocess, 45.1ms inference, 3.7ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 45.0ms\n","Speed: 9.3ms preprocess, 45.0ms inference, 3.9ms postprocess per image at shape (1, 3, 1024, 1024)\n","\n","0: 1024x1024 44.2ms\n","Speed: 9.8ms preprocess, 44.2ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"]}]},{"cell_type":"code","source":["# Объединяем все DataFrame'ы в один\n","concatenated_df = pd.concat(final_results)\n","\n","# Группируем по 'Library' и вычисляем среднее для каждой группы\n","average_df = concatenated_df.groupby('Format').mean()\n","average_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"hCpDRuWTqvbB","outputId":"0ce25c6b-807b-4ebf-ab9b-6e8b78d0763b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             Total Inference Time (s)  Peak Memory Usage (MB)  \\\n","Library                                                         \n","ONNX                       591.436150             3551.074219   \n","PyTorch                    441.928984             3092.441406   \n","TensorRT                   403.528695             3880.816406   \n","TorchScript                437.823136             3627.671875   \n","\n","             Avg. Preprocess Time (ms)  Avg. Inference Time (ms)  \\\n","Library                                                            \n","ONNX                         10.242347                 73.124912   \n","PyTorch                      10.119148                 51.011471   \n","TensorRT                     10.354370                 45.392624   \n","TorchScript                   9.965739                 50.894499   \n","\n","             Avg. Postprocess Time (ms)  \n","Library                                  \n","ONNX                           2.923396  \n","PyTorch                        3.346911  \n","TensorRT                       3.127925  \n","TorchScript                    3.018247  "],"text/html":["\n","  <div id=\"df-0f02e031-cb15-482d-821c-e71b6d2e5427\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Total Inference Time (s)</th>\n","      <th>Peak Memory Usage (MB)</th>\n","      <th>Avg. Preprocess Time (ms)</th>\n","      <th>Avg. Inference Time (ms)</th>\n","      <th>Avg. Postprocess Time (ms)</th>\n","    </tr>\n","    <tr>\n","      <th>Library</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ONNX</th>\n","      <td>591.436150</td>\n","      <td>3551.074219</td>\n","      <td>10.242347</td>\n","      <td>73.124912</td>\n","      <td>2.923396</td>\n","    </tr>\n","    <tr>\n","      <th>PyTorch</th>\n","      <td>441.928984</td>\n","      <td>3092.441406</td>\n","      <td>10.119148</td>\n","      <td>51.011471</td>\n","      <td>3.346911</td>\n","    </tr>\n","    <tr>\n","      <th>TensorRT</th>\n","      <td>403.528695</td>\n","      <td>3880.816406</td>\n","      <td>10.354370</td>\n","      <td>45.392624</td>\n","      <td>3.127925</td>\n","    </tr>\n","    <tr>\n","      <th>TorchScript</th>\n","      <td>437.823136</td>\n","      <td>3627.671875</td>\n","      <td>9.965739</td>\n","      <td>50.894499</td>\n","      <td>3.018247</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f02e031-cb15-482d-821c-e71b6d2e5427')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0f02e031-cb15-482d-821c-e71b6d2e5427 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0f02e031-cb15-482d-821c-e71b6d2e5427');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e6a3642f-2f3c-4bdb-8b7d-cecf55f7cd45\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6a3642f-2f3c-4bdb-8b7d-cecf55f7cd45')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e6a3642f-2f3c-4bdb-8b7d-cecf55f7cd45 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"average_df","summary":"{\n  \"name\": \"average_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Library\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"PyTorch\",\n          \"TorchScript\",\n          \"ONNX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Inference Time (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 83.6291967690113,\n        \"min\": 403.52869546801867,\n        \"max\": 591.4361495949884,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          441.9289840309991,\n          437.8231359659983,\n          591.4361495949884\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Peak Memory Usage (MB)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 328.76411463579086,\n        \"min\": 3092.44140625,\n        \"max\": 3880.81640625,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3092.44140625,\n          3627.671875,\n          3551.07421875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Avg. Preprocess Time (ms)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16686769794181705,\n        \"min\": 9.965739020862735,\n        \"max\": 10.35437046002762,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          10.119147523051993,\n          9.965739020862735,\n          10.242347362123052\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Avg. Inference Time (ms)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.295429681030212,\n        \"min\": 45.39262352246639,\n        \"max\": 73.12491181295545,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          51.01147091493994,\n          50.8944991702652,\n          73.12491181295545\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Avg. Postprocess Time (ms)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18216236828018395,\n        \"min\": 2.923395857747376,\n        \"max\": 3.3469106006714613,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.3469106006714613,\n          3.018246776189968,\n          2.923395857747376\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["average_df.to_csv('average_df_gpu.csv', index=True)"],"metadata":{"id":"PlZOk3uoHtk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","with open(\"all_raw_results_gpu.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(all_raw_results, f, ensure_ascii=False, indent=4)  # Красиво форматируем"],"metadata":{"id":"D-lacy7lWer9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dXppxrbHutQD"},"execution_count":null,"outputs":[]}]}